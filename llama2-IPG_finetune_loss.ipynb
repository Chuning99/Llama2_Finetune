{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuXIFTFapAMI"
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "# from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "# from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "# fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "#     state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "#     optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "# )\n",
    "\n",
    "# accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcE4NTeFyRgd"
   },
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='/home/z/Music/LLAMA/llama/IPG/datasets/new_train_data.json', split='train')  \n",
    "eval_dataset = load_dataset('json', data_files='/home/z/Music/LLAMA/llama/IPG/datasets/new_test_data.json', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting prompts\n",
    "\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### The following is a note by Eevee the Dog: {example['note']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Job Description:\\n{example['text']}\\n\\n### Required Skills:\\n{example['skills']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Llama 2 13B - `meta-llama/Llama-2-13b-hf` - using 4 or 8-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E0Nl5mWL0k2T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "# from bitsandbytes.optim import BitsAndBytesConfig\n",
    "\n",
    "# base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "# # 使用8-bit量化配置\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=False,  # 关闭4-bit加载\n",
    "#     bnb_4bit_compute_dtype=torch.float32  # 使用标准的32-bit浮点数\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "haSUDD9HyRgf"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    "    return_tensors=\"pt\" \n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     return tokenizer(formatting_func(prompt))\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    text = prompt['text']\n",
    "    skills = ', '.join(prompt['skills'])\n",
    "    not_skills = ', '.join(prompt['not_skills'])\n",
    "\n",
    "    # 对 text 进行标记化\n",
    "    text_encoding = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=1300)\n",
    "\n",
    "    # 对 skills 和 not_skills 进行标记化\n",
    "    skills_encoding = tokenizer(skills, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=100)\n",
    "    not_skills_encoding = tokenizer(not_skills, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=100)\n",
    "\n",
    "    return {\n",
    "        'input_ids': text_encoding['input_ids'].flatten(),\n",
    "        'attention_mask': text_encoding['attention_mask'].flatten(),\n",
    "        'skills_input_ids': skills_encoding['input_ids'].flatten(),\n",
    "        'skills_attention_mask': skills_encoding['attention_mask'].flatten(),\n",
    "        'not_skills_input_ids': not_skills_encoding['input_ids'].flatten(),\n",
    "        'not_skills_attention_mask': not_skills_encoding['attention_mask'].flatten()\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     text = prompt['text']\n",
    "#     skills = ', '.join(prompt['skills'])\n",
    "#     not_skills = ', '.join(prompt['not_skills'])\n",
    "\n",
    "#     # 对 text 进行标记化\n",
    "#     text_encoding = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=1300)\n",
    "\n",
    "#     # 标记化 skills 和 not_skills\n",
    "#     # 请根据您的模型需求调整这里的处理方式\n",
    "#     skills_encoding = tokenizer(skills, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=100)\n",
    "#     not_skills_encoding = tokenizer(not_skills, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=100)\n",
    "\n",
    "#     return {\n",
    "#         'input_ids': text_encoding['input_ids'].flatten(),\n",
    "#         'attention_mask': text_encoding['attention_mask'].flatten(),\n",
    "#         'skills_input_ids': skills_encoding['input_ids'].flatten(),\n",
    "#         'skills_attention_mask': skills_encoding['attention_mask'].flatten(),\n",
    "#         'not_skills_input_ids': not_skills_encoding['input_ids'].flatten(),\n",
    "#         'not_skills_attention_mask': not_skills_encoding['attention_mask'].flatten()\n",
    "#     }\n",
    "    # return {\n",
    "    #     \"input_ids_to_learn\": features_to_learn['input_ids'],\n",
    "    #     \"attention_mask_to_learn\": features_to_learn['attention_mask'],\n",
    "    #     \"input_ids_to_avoid\": features_to_avoid['input_ids'],\n",
    "    #     \"attention_mask_to_avoid\": features_to_avoid['attention_mask']\n",
    "    #     # 可以添加更多字段，如 labels\n",
    "    # }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skills': ['leadership', 'sql', 'planning', 'devops', 'marketing', 'big data', 'crm', 'linkedin', 'agile', 'mentoring', 'cloud', 'web services', 'interpersonal', 'ai', 'communication', 'cost control', 'transformation', 'gcp', 'industrial', 'ml', 'saas', 'system', 'aws', 'business development', 'nlp', 'machine learning', 'multi-vendor teams. advanced talent', 'automation'], 'text': 'capability of building team, brand from the groud\\nwide connection with potential clients\\ngood understanding of AI, ML', 'id': '8247', 'not_skills': ['salesforce', 'mac', 'solution selling', 'spark', 'project management', 'vmware', 'hadoop', 'microsoft azure', 'account management', 'python', 'ios', 'oracle', 'support', 'linux', 'android', 'kubernetes'], 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2117, 3097, 310, 5214, 3815, 29892, 14982, 515, 278, 867, 2736, 13, 8157, 3957, 411, 7037, 13154, 13, 16773, 8004, 310, 319, 29902, 29892, 23158, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'skills_input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 26001, 29892, 4576, 29892, 18987, 29892, 2906, 3554, 29892, 9999, 292, 29892, 4802, 848, 29892, 274, 1758, 29892, 9024, 262, 29892, 946, 488, 29892, 6042, 8253, 29892, 9570, 29892, 1856, 5786, 29892, 1006, 10532, 284, 29892, 7468, 29892, 12084, 29892, 3438, 2761, 29892, 13852, 29892, 330, 6814, 29892, 18408, 29892, 286, 29880, 29892, 872, 294, 29892, 1788, 29892, 25879, 29892, 5381, 5849, 29892, 302, 22833, 29892, 4933, 6509, 29892, 2473, 29899, 19167, 10907, 29889, 12862, 24242, 29892, 3345, 362, 2], 'skills_attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'not_skills_input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 16538, 10118, 29892, 5825, 29892, 1650, 269, 7807, 29892, 16267, 29892, 2060, 10643, 29892, 22419, 2519, 29892, 750, 26793, 29892, 286, 2995, 15699, 29892, 3633, 10643, 29892, 3017, 29892, 10615, 29892, 17919, 29892, 2304, 29892, 10542, 29892, 1442, 29892, 413, 17547, 2], 'not_skills_attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[6])  # 检查第一个样本是否包含 'skills' 和 'not_skills'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2217\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenize_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs. \n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 1200\n",
    "# def generate_and_tokenize_prompt2(prompt):\n",
    "#     result = tokenizer(\n",
    "#         formatting_func(prompt),\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 1500  # 设置最大长度\n",
    "\n",
    "# def generate_and_tokenize_prompt2(prompt):\n",
    "#     formatted_prompt = formatting_func(prompt)\n",
    "#     result = tokenizer(\n",
    "#         formatted_prompt,\n",
    "#         truncation=True,  \n",
    "#         max_length=max_length,  \n",
    "#         padding=\"longest\",  \n",
    "#         return_tensors=\"pt\",  \n",
    "#         return_attention_mask=True  \n",
    "#     )\n",
    "    \n",
    "#     # 复制input_ids到labels，用于模型训练的目标\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "#     return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "# tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skills': ['leadership', 'php', 'tensorflow', 'sql', 'c++', 'matlab', 'seaborn', 'event management', 'matplotlib', 'git', 'powerpoint', 'riak', 'c sharp', 'scikit-learn', 'excel', 'rstudio', 'jdbc', 'libsvm', 'writing', 'pytorch', 'javascript', 'python', 'latex', 'node.js', 'c#', 'jupyter notebook'], 'text': 'Located in the Bay Area, Geneus Tech Inc. is a subsidiary/branch research center of its parent company in China, which is dedicated to the development and commercial application of 4th generation (nanopore-based) gene sequencing solutions, including devices, IC-MEMS chips, reagents, algorithms and software systems. The core technology platform is based on the detection and processing of characteristic current signals generated by the interaction between nanopore biosensors and nucleic acid bases. Thus, high-throughput sequencing of nucleic acids can be achieved at the single-molecule level through a highly integrated chip system.\\nThis cutting-edge technology is highly expected to revolutionize the current sequencing filed by providing results with significantly higher data quality and lower cost as well as much higher convenience and timeliness. The parent company in China just hit a major milestone to launch the alpha version of our first generation product, making us highly encouraged and excited to continue our R&D, mature and upgrade the products, open up a blue ocean market and eventually reshape the current gene sequencing field to facilitate the new era of precision/personalized medicine. \\xa0 \\xa0\\nHere in the bay area research center, we are planning to attract talents interested in this frontier to take our R&D to a higher level. You will be provided with competitive salary, benefit and equity, as well as a bright career path and a warm, dynamic working environment.\\n\\xa0\\nResponsibilities:1. Develop, optimize and iterate algorithms to process raw current signals and convert them into nucleic acid sequences with high accuracy and efficiency.\\xa02. Develop, optimize and iterate algorithms for sequence alignment and obtain consensus sequence with high accuracy3. Advanced research of algorithm methodologies for single-molecule sequencing signals4. Work closely with the bioinformatics team and software team to integrate related', 'id': '20315', 'not_skills': ['keras', 'opencv', 'numpy', 'computer vision', 'anomaly detection', 'solidworks', 'cntk', 'adobe illustrator', 'pca', 'horovod', 'labview', 'maple', 'deep learning', 'selenium', 'svm', 'swift', 'cnn', 'adobe photoshop', 'igor pro', 'pandas', 'nlp', 'tensorfelow'], 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 5976, 630, 297, 278, 6211, 18320, 29892, 15350, 375, 1920, 305, 9266, 29889, 338, 263, 11684, 8819, 653, 29914, 17519, 5925, 4818, 310, 967, 3847, 5001, 297, 7551, 29892, 607, 338, 16955, 304, 278, 5849, 322, 12128, 2280, 310, 29871, 29946, 386, 12623, 313, 13707, 459, 487, 29899, 6707, 29897, 18530, 8617, 16750, 6851, 29892, 3704, 9224, 29892, 18340, 29899, 2303, 4345, 521, 4512, 29892, 337, 351, 1237, 29892, 14009, 322, 7047, 6757, 29889, 450, 7136, 15483, 7481, 338, 2729, 373, 278, 15326, 322, 9068, 310, 17443, 1857, 18470, 5759, 491, 278, 14881, 1546, 23432, 459, 487, 289, 2363, 575, 943, 322, 22699, 293, 22193, 22561, 29889, 6549, 29892, 1880, 29899, 20678, 649, 8617, 16750, 310, 22699, 293, 1274, 4841, 508, 367, 14363, 472, 278, 2323, 29899, 29885, 1772, 29883, 1297, 3233, 1549, 263, 10712, 23387, 29830, 1788, 29889, 13, 4013, 28967, 29899, 12864, 15483, 338, 10712, 3806, 304, 19479, 675, 278, 1857, 8617, 16750, 934, 29881, 491, 13138, 2582, 411, 16951, 6133, 848, 11029, 322, 5224, 3438, 408, 1532, 408, 1568, 6133, 29703, 322, 5335, 295, 3335, 29889, 450, 3847, 5001, 297, 7551, 925, 7124, 263, 4655, 2316, 27744, 304, 6826, 278, 15595, 1873, 310, 1749, 937, 12623, 3234, 29892, 3907, 502, 10712, 18443, 287, 322, 24173, 304, 6773, 1749, 390, 29987, 29928, 29892, 286, 1535, 322, 14955, 278, 9316, 29892, 1722, 701, 263, 7254, 23474, 9999, 322, 10201, 620, 14443, 278, 1857, 18530, 8617, 16750, 1746, 304, 16089, 10388, 278, 716, 3152, 310, 16716, 29914, 10532, 284, 1891, 26602, 29889, 20246, 20246, 13, 10605, 297, 278, 23041, 4038, 5925, 4818, 29892, 591, 526, 18987, 304, 13978, 5969, 1237, 8852, 297, 445, 4565, 631, 304, 2125, 1749, 390, 29987, 29928, 304, 263, 6133, 3233, 29889, 887, 674, 367, 4944, 411, 5100, 3321, 4497, 653, 29892, 14169, 322, 1592, 537, 29892, 408, 1532, 408, 263, 11785, 6413, 2224, 322, 263, 14294, 29892, 7343, 1985, 5177, 29889, 13, 30081, 13, 1666, 29886, 787, 747, 9770, 29901, 29896, 29889, 10682, 29892, 24656, 322, 13649, 14009, 304, 1889, 10650, 1857, 18470, 322, 3588, 963, 964, 22699, 293, 22193, 15602, 411, 1880, 13600, 322, 19201, 29889, 30081, 29906, 29889, 10682, 29892, 24656, 322, 13649, 14009, 363, 5665, 22239, 322, 4017, 1136, 8841, 5665, 411, 1880, 13600, 29941, 29889, 29287, 5925, 310, 5687, 1158, 11763, 363, 2323, 29899, 29885, 1772, 29883, 1297, 8617, 16750, 18470, 29946, 29889, 5244, 16467, 411, 278, 17799, 262, 4830, 1199, 3815, 322, 7047, 3815, 304, 22782, 4475, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'skills_input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 26001, 29892, 3989, 29892, 26110, 29892, 4576, 29892, 274, 10024, 1775, 8205, 29892, 409, 370, 1398, 29892, 1741, 10643, 29892, 22889, 29892, 6315, 29892, 3081, 3149, 29892, 364, 24061, 29892, 274, 15301, 29892, 4560, 7354, 29899, 19668, 29892, 10616, 29892, 364, 12073, 29892, 432, 11140, 29892, 4303, 4501, 29885, 29892, 5007, 29892, 282, 3637, 25350, 29892, 3513, 29892, 3017, 29892, 5683, 29916, 29892, 2943, 29889, 1315, 29892, 274, 6552, 432, 786, 25547, 451, 19273, 2], 'skills_attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'not_skills_input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 13023, 294, 29892, 1722, 11023, 29892, 12655, 29892, 6601, 18551, 29892, 29342, 14997, 15326, 29892, 7773, 13129, 29892, 274, 593, 29895, 29892, 594, 16945, 8632, 1061, 29892, 282, 1113, 29892, 4029, 586, 397, 29892, 9775, 1493, 29892, 2910, 280, 29892, 6483, 6509, 29892, 18866, 29892, 3731, 29885, 29892, 12086, 29892, 274, 15755, 29892, 594, 16945, 6731, 10578, 459, 29892, 8919, 272, 410, 29892, 11701, 29892, 302, 22833, 29892, 25187, 4877, 295, 340, 2], 'not_skills_attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\" ### The job description: \\nPosition: Digital Manager/Director (Full time)\\n \\nAbout Red Harp\\nRed Harp is an exciting new kind of agency for B2C brands looking to expand into new global markets. Our ‘Total Go-to-Market (GTM) Solution’ is a unique service offering encompassing branding, marketing, and commercial enablement to help companies quickly and effectively establish themselves in new markets. We’re a startup that is innovating the way global commerce works.\\n \\nAbout the Team\\nThe Branding & Marketing Team at Red Harp partners with overseas clients on both brand and performance marketing efforts tailored to the US market – to drive awareness and favorability for long-term interest and purchase consideration, as well as in-market activation for short-term sales opportunities.\\n \\nPosition Overview\\nRed Harp is looking for an enthusiastic candidate with first-hand experience in social media marketing to join the Team as our lead social community manager/director. This is a full-time employment position in our headquarters office located in Sunnyvale, CA.\\n \\nThe ideal candidate will lead management of social communications for clients, with responsibility for keeping existing audiences engaged while expanding the client’s reach and influence through strategic organic and paid activities. This will be a critical role for establishing and communicating our clients’ brand promise in the US market.\\n \\nResponsibilities\\n\\nDevelop and deploy a defined go-to market strategy for reaching and engaging target audiences\\n\\nHelp manage editorial calendars for our clients to support multiple ongoing social initiatives – on a quarterly, monthly, weekly, and daily basis (planned campaigns, as well as timely spontaneous activities)\\nCoordinate with internal colleagues, contractors, partners, and client teams on the development and execution of social media strategy, content, creative, and other deliverables\\nAuthor and deploy posts on clients’ owned social channels on a daily basis – across Twitter, Facebook, Instagram, blogs, and other platforms\\n\\n\\n\\n \\n\\nEngage in branded conversations\\n\\nIdentify and create strategic opportunities for the brand to engage target audiences in a two-way dialog – conversation prompts, questions, polls, live events, and other tactics\\nHelp reach out to potential influencers and brand ambassadors with the aim of creating opportunities to produce content, extend reach, and generate earned media exposure\\n\\n\\n\\n \\n\\nManage paid social media activities\\n\\nCreate paid media plans to drive both A) brand engagement, and B) traffic to e-commerce channels\\nManage ads on a daily basis to maximize the efficiency and effectiveness of client budgets\\n\\n\\n\\n \\n\\nBe data-driven\\n\\nServe as the eyes and ears for the brand, monitoring and reporting on user sentiment and interests, brand engagement, competitive activities, etc.\\nAggregate performance data into a single comprehensive reporting tool\\nAnalyze data and synthesize insights to inform future and ongoing social activities –identifying opportunities to optimize activities every week\\nAssist with the creation of client reports and presentations\\n\\n\\n\\n\\nRequirements\\n\\n5+ years of experience working for a brand, agency, or media company, with 3+ years of direct experience managing social media communities\\nWorking knowledge of content marketing, paid media/advertising, influencer marketing, and other marketing disciplines\\nAbility to engage intelligently with B2C audiences – to translate the brand’s messaging and value propositions into thought-led communications that address the audience’s needs, challenges, and interests\\nAdvanced creative writing experience with an ability to understand the client’s brand tone and translate that into effective social posts that drive desired actions (engagement, clicks, etc.)\\nAvailability to work extended hours – including limited time on weekends – to provide the client with comprehensive monitoring and coverage throughout the day\\nStrong project management and problem-solving skills with an ability to create project plans and manage through seamless execution\\nAbility to measure, track, and report on campaign effectiveness, and optimize along the way to ensure campaign success\\nAnalytical expertise in assessing social media trends and campaign analysis\\nExperience working with social media tools such as Buffer, Hootsuite, Sprinkler, etc.\\nA bachelor's degree is required\\n\\n \\nThe ideal candidate must be comfortable working in a fast-paced environment, be a responsible and reliable Team member, and share our enthusiasm for our work helping brands achieve success in new markets.\\n \\n \"\n",
    " \\n \n",
    "\n",
    "\n",
    "### The skills in keywords that the job needs and not needs:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NidIuFXMyRgi"
   },
   "outputs": [],
   "source": [
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ybeyl20n3dYH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26214400 || all params: 6698193920 || trainable%: 0.391365199531279\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        # \"v_proj\",\n",
    "        # \"o_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"down_proj\",\n",
    "        # \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "#model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 5120)\n",
      "        (layers): ModuleList(\n",
      "          (0-39): 40 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeruiwang\u001b[0m (\u001b[33mzeruiw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"IPG-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5.0 loss function setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# class IPGkeywordsT(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # 提取输入数据\n",
    "#         text_input_ids = inputs.get(\"input_ids\")\n",
    "#         text_attention_mask = inputs.get(\"attention_mask\")\n",
    "#         labels = inputs.get(\"labels\")\n",
    "\n",
    "#         # 从 inputs 中获取 skills 和 not_skills 的 input_ids 和 attention_mask\n",
    "#         skills_input_ids = inputs.get(\"skills_input_ids\")\n",
    "#         skills_attention_mask = inputs.get(\"skills_attention_mask\")\n",
    "#         not_skills_input_ids = inputs.get(\"not_skills_input_ids\")\n",
    "#         not_skills_attention_mask = inputs.get(\"not_skills_attention_mask\")\n",
    "\n",
    "#         # 将标签左移一位\n",
    "#         shifted_labels = torch.roll(labels, -1, dims=-1)\n",
    "        \n",
    "#         # 使用模型进行推理\n",
    "#         positive_outputs = model(input_ids=text_input_ids, attention_mask=text_attention_mask, labels=shifted_labels)\n",
    "#         positive_loss = positive_outputs.loss\n",
    "\n",
    "#         negative_outputs_learn = model(input_ids=skills_input_ids, attention_mask=skills_attention_mask, labels=shifted_labels)\n",
    "#         negative_outputs_avoid = model(input_ids=not_skills_input_ids, attention_mask=not_skills_attention_mask, labels=shifted_labels)\n",
    "\n",
    "#         # 计算负向损失\n",
    "#         negative_loss_learn = negative_outputs_learn.loss\n",
    "#         negative_loss_avoid = negative_outputs_avoid.loss\n",
    "\n",
    "#         # 定义权重\n",
    "#         w_like = 1\n",
    "#         w_dislike = 0.991\n",
    "#         w_l = 0.99\n",
    "\n",
    "#         # 计算总损失\n",
    "#         total_loss = w_like * positive_loss - w_dislike * (negative_loss_learn + negative_loss_avoid) + w_l * torch.sqrt(positive_loss ** 2 + (negative_loss_learn + negative_loss_avoid) ** 2)\n",
    "\n",
    "#         if return_outputs:\n",
    "#             return (total_loss, positive_outputs)\n",
    "#         return total_loss\n",
    "\n",
    "# class IPGkeywordsT(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # 提取输入数据\n",
    "#         text_input_ids = inputs.get(\"input_ids\")\n",
    "#         text_attention_mask = inputs.get(\"attention_mask\")\n",
    "#         labels = inputs.get(\"labels\")\n",
    "\n",
    "#         # 打印输入尺寸和标签尺寸\n",
    "#         print(\"Input IDs shape:\", text_input_ids.shape)\n",
    "#         print(\"Attention mask shape:\", text_attention_mask.shape)\n",
    "#         print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "#         # 从 inputs 中获取 skills 和 not_skills 的 input_ids 和 attention_mask\n",
    "#         skills_input_ids = inputs.get(\"skills_input_ids\")\n",
    "#         skills_attention_mask = inputs.get(\"skills_attention_mask\")\n",
    "#         not_skills_input_ids = inputs.get(\"not_skills_input_ids\")\n",
    "#         not_skills_attention_mask = inputs.get(\"not_skills_attention_mask\")\n",
    "\n",
    "#         # 使用模型进行推理\n",
    "#         # positive_outputs = model(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "#         # positive_logits = positive_outputs.logits\n",
    "#         positive_outputs = model(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "\n",
    "#         positive_logits = positive_outputs.logits\n",
    "\n",
    "#         # 打印正面输出的 logits 尺寸\n",
    "#         print(\"Positive logits shape:\", positive_logits.shape)\n",
    "\n",
    "#         positive_loss = F.cross_entropy(positive_logits.view(-1, positive_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "#         negative_outputs_learn = model(input_ids=skills_input_ids, attention_mask=skills_attention_mask)\n",
    "#         negative_outputs_avoid = model(input_ids=not_skills_input_ids, attention_mask=not_skills_attention_mask)\n",
    "\n",
    "#         # 打印负面输出的 logits 尺寸\n",
    "#         print(\"Negative learn logits shape:\", negative_outputs_learn.logits.shape)\n",
    "#         print(\"Negative avoid logits shape:\", negative_outputs_avoid.logits.shape)\n",
    "\n",
    "#         # 计算负向损失\n",
    "#         negative_loss = self.calculate_negative_loss(negative_outputs_learn.logits, negative_outputs_avoid.logits, labels)\n",
    "\n",
    "#         # 计算总损失\n",
    "#         alpha = 0.3\n",
    "#         total_loss = positive_loss - alpha * negative_loss\n",
    "\n",
    "#         # 打印总损失\n",
    "#         print(\"Total loss:\", total_loss.item())\n",
    "\n",
    "#         return (total_loss, positive_outputs) if return_outputs else total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class IPGkeywordsT(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "#         features_to_learn = inputs.get(\"features_to_learn\")\n",
    "#         features_to_avoid = inputs.get(\"features_to_avoid\")\n",
    "#         labels = inputs.get(\"labels\")\n",
    "\n",
    "#         positive_outputs = model(features_to_learn)\n",
    "#         positive_logits = positive_outputs.logits\n",
    "#         positive_loss = F.cross_entropy(positive_logits, labels)\n",
    "\n",
    "#         negative_outputs = model(features_to_avoid)\n",
    "#         negative_logits = negative_outputs.logits\n",
    "#         negative_loss = self.calculate_negative_loss(positive_logits, negative_logits, labels)\n",
    "\n",
    "#         alpha = 0.3\n",
    "#         total_loss = positive_loss - alpha * negative_loss\n",
    "\n",
    "#         if return_outputs:\n",
    "#             return (total_loss, positive_outputs)\n",
    "#         return total_loss\n",
    "\n",
    "    # def calculate_negative_loss(self, positive_logits, negative_logits, labels):\n",
    "    #     # 假设 loss_like 和 loss_dislike 基于交叉熵\n",
    "    #     loss_like = F.cross_entropy(positive_logits, labels)\n",
    "    #     loss_dislike = F.cross_entropy(negative_logits, labels)\n",
    "\n",
    "    #     # 定义权重\n",
    "    #     w_like = 1\n",
    "    #     w_dislike = 0.991\n",
    "    #     w_l = 0.99\n",
    "\n",
    "    #     # 计算负向损失\n",
    "    #     loss = w_like * loss_like - w_dislike * loss_dislike + w_l * torch.sqrt(loss_like ** 2 + loss_dislike ** 2)\n",
    "    #     return loss\n",
    "\n",
    "\n",
    "    # epsilon: float = 0.1\n",
    "    # ignore_index: int = -100\n",
    "    # def __call__(self, model_output, labels, shift_labels=False):\n",
    "    #     # print('\\n\\n__call____call__')\n",
    "    #     labels, not_labels = torch.chunk(labels, 2, dim=1)\n",
    "    #     temp_tensor = torch.full((labels.shape[0], 256), -100).cuda()\n",
    "    #     labels = torch.cat((labels, temp_tensor), dim=1)\n",
    "    #     not_labels = torch.cat((not_labels, temp_tensor), dim=1)\n",
    "    #     logits = model_output[\"logits\"] if isinstance(model_output, dict) else model_output[0]\n",
    "    #     if shift_labels:  # default is False\n",
    "    #         logits = logits[..., :-1, :].contiguous()\n",
    "    #         labels = labels[..., 1:].contiguous()\n",
    "    #         not_labels = not_labels[..., 1:].contiguous()\n",
    "    #     # print('shift_labels',shift_labels)\n",
    "    #     log_probs = -torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    #     # print(f'logits.shape', logits.shape,logits[0][0])\n",
    "    #     # print(f'log_probs.shape', log_probs.shape,log_probs[0][0])\n",
    "    #     losses = []\n",
    "    #     for i, labels_ in enumerate((labels, not_labels)):\n",
    "    #         if labels_.dim() == log_probs.dim() - 1:\n",
    "    #             labels_ = labels_.unsqueeze(-1)\n",
    "    #         # print(f'labels{i}.shape', labels.shape,labels[0][:10])\n",
    "    #         padding_mask = labels_.eq(self.ignore_index)\n",
    "    #         # In case the ignore_index is -100, the gather will fail, so we replace labels_ by 0. The padding_mask\n",
    "    #         # will ignore them in any case.\n",
    "    #         labels_ = torch.clamp(labels_, min=0)\n",
    "    #         nll_loss = log_probs.gather(dim=-1, index=labels_)\n",
    "    #         nll_loss_shape = list(nll_loss.shape)[1]\n",
    "    #         # works for fp16 input tensor too, by internally upcasting it to fp32\n",
    "    #         smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n",
    "    #         nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "    #         smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
    "    #         # print(f'smoothed_loss.shape', smoothed_loss.shape,smoothed_loss[0][:10])\n",
    "    #         # print(f'nll_loss.shape', nll_loss.shape,nll_loss[0][:10])\n",
    "    #         # Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):\n",
    "    #         num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
    "    #         nll_loss = nll_loss.sum() / num_active_elements\n",
    "    #         smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
    "    #         losses.append((1 - self.epsilon) * nll_loss + self.epsilon * smoothed_loss)\n",
    "    #         # print('loss',losses[0])\n",
    "    #         # return losses[0]\n",
    "    #     loss_like, loss_dislike = losses\n",
    "    #     #  如果  loss_dis_like 大于 loss_1，正常，最终loss小\n",
    "    #     #  如果  loss_dis_like 小于 loss_1，不正常，最终 loss 大\n",
    "    #     # loss_1 = 10 ，loss_dis_like=100 or 1\n",
    "    #     w_like = 1\n",
    "    #     w_dislike = 0.991\n",
    "    #     w_l = 0.99\n",
    "    #     loss = w_like * loss_like - w_dislike * loss_dislike + w_l * torch.sqrt(loss_like ** 2 + loss_dislike ** 2)\n",
    "    #     print('losses', losses, ' = ', loss)\n",
    "    #     return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jq0nX33BmfaC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/z/Music/LLAMA/llama/IPG/wandb/run-20231110_015526-44g4sw3h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zeruiw/IPG-finetune/runs/44g4sw3h' target=\"_blank\">llama2-13b-IPG-finetune-2023-11-10-01-55</a></strong> to <a href='https://wandb.ai/zeruiw/IPG-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zeruiw/IPG-finetune' target=\"_blank\">https://wandb.ai/zeruiw/IPG-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zeruiw/IPG-finetune/runs/44g4sw3h' target=\"_blank\">https://wandb.ai/zeruiw/IPG-finetune/runs/44g4sw3h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f25e2584c554060885ebccc324ba0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/z/miniconda3/envs/llama/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb 单元格 45\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trainer \u001b[39m=\u001b[39m IPGkeywordsT(    \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[1;32m/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb 单元格 45\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m positive_outputs \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39mtext_input_ids, attention_mask\u001b[39m=\u001b[39mtext_attention_mask, labels\u001b[39m=\u001b[39mshifted_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m positive_loss \u001b[39m=\u001b[39m positive_outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m negative_outputs_learn \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39;49mskills_input_ids, attention_mask\u001b[39m=\u001b[39;49mskills_attention_mask, labels\u001b[39m=\u001b[39;49mshifted_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m negative_outputs_avoid \u001b[39m=\u001b[39m model(input_ids\u001b[39m=\u001b[39mnot_skills_input_ids, attention_mask\u001b[39m=\u001b[39mnot_skills_attention_mask, labels\u001b[39m=\u001b[39mshifted_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# 计算负向损失\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/utils/operations.py:662\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/utils/operations.py:650\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/peft/peft_model.py:977\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    967\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    968\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    969\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    975\u001b[0m         )\n\u001b[0;32m--> 977\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m    978\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    979\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    980\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    981\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    982\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    983\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    984\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    985\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    986\u001b[0m     )\n\u001b[1;32m    988\u001b[0m batch_size \u001b[39m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    989\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    990\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:106\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1035\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1036\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1037\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1038\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1039\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1040\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1041\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1042\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1043\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1044\u001b[0m )\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:865\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    863\u001b[0m     batch_size, seq_length \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    864\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 865\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    867\u001b[0m past_key_values_length \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    868\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7fabecb81570>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fabed6a44f0, execution_count=26 error_before_exec=None error_in_exec=You have to specify either input_ids or inputs_embeds info=<ExecutionInfo object at 7fabed6a76a0, raw_cell=\"import transformers\n",
      "from datetime import datetime\n",
      "..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/z/Music/LLAMA/llama/IPG/llama2-IPG_finetune_loss.ipynb#X56sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"IPG-finetune\"\n",
    "base_model_name = \"llama2-13b\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# trainer = transformers.Trainer(\n",
    "trainer = IPGkeywordsT(    \n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        #warmup_steps=2,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=2000,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=100,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=100,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        #report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

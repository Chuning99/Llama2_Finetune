{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SKSnF016yRgp"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "# base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "# # bnb_config = BitsAndBytesConfig(\n",
    "# #     load_in_4bit=True,\n",
    "# #     bnb_4bit_use_double_quant=True,\n",
    "# #     bnb_4bit_quant_type=\"nf4\",\n",
    "# #     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# # )\n",
    "\n",
    "\n",
    "# #model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n",
    "\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload= True )\n",
    "\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_id,  \n",
    "#     quantization_config=bnb_config,  # Same quantization config as before\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n",
    "\n",
    "# # import torch\n",
    "# # from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "# # # 加载基础模型和tokenizer\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "# # base_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuning/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/chuning/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.62s/it]\n",
      "/home/chuning/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Define the base model ID\n",
    "base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "# Create a BitsAndBytesConfig object with the corrected settings\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    load_in_8bit_fp32_cpu_offload=True  # Set as suggested in the error\n",
    ")\n",
    "\n",
    "# Load the base model with the updated quantization configuration\n",
    "# Adjust 'device_map' based on your system's GPU configuration\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  \n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a contract RTL design engineer, you will work as part of a GPU IP design team. This is an open-ended performance-driven position where the contractor will be tasked with supporting RTL design of various sub-blocks of the GPU graphics pipeline for a GPU targeted to the mobile market as well as other markets. Significant architectural, as well as RTL design experience. Key responsibilities include: •Micro-architecture and RTL definition •Partner with the physical design team to resolve implementation level details •Work closely with design verification to test plan and otherwise ensure proper functionality requirements •Deliver quality micro-architectural level documentation •Produce quality RTL on schedule meeting PPA goals. Optimize design for low power.\n",
      "\n",
      "\n",
      "Requirements\n",
      "Minimum requirements: •BSEE, Computer Engineer or comparable and 3 + years of experience •Experience driving the RTL design of various digital sub-blocks of GPU /CPU high-performance digital designs •Demonstrated experience of successful Architectural through RTL design experience on high-performance digital designs The preferred candidate will possess the following: •Good written and verbal communication skills •Pixel Pipe or GPU experience preferred •Low Power Design experience preferred\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "As a contract RTL design engineer, you will work as part of a GPU IP design team. This is an open-ended performance-driven position where the contractor will be tasked with supporting RTL design of various sub-blocks of the GPU graphics pipeline for a GPU targeted to the mobile market as well as other markets. Significant architectural, as well as RTL design experience. Key responsibilities include: •Micro-architecture and RTL definition •Partner with the physical design team to resolve implementation level details •Work closely with design verification to test plan and otherwise ensure proper functionality requirements •Deliver quality micro-architectural level documentation •Produce quality RTL on schedule meeting PPA goals. Optimize design for low power.\\n\\n\\nRequirements\\nMinimum requirements: •BSEE, Computer Engineer or comparable and 3 + years of experience •Experience driving the RTL design of various digital sub-blocks of GPU /CPU high-performance digital designs •Demonstrated experience of successful Architectural through RTL design experience on high-performance digital designs The preferred candidate will possess the following: •Good written and verbal communication skills •Pixel Pipe or GPU experience preferred •Low Power Design experience preferred\"\n",
    "\n",
    "\"\"\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BxOhAiqyRgp"
   },
   "source": [
    "Now load the QLoRA adapter from the appropriate checkpoint directory, i.e. the best performing model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"/home/chuning/Downloads/llama/llama2-13b-IPG-finetune/checkpoint-800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lMkVNEUvyRgp"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# ft_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=300)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "  \n",
    "eval_dataset = load_dataset('json', data_files='/home/chuning/Downloads/llama/test/new_test_data.json', split='train')\n",
    "\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### The job description: {example['text']}\\n ### The skills: \"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_finetune_model(model_id):\n",
    "\n",
    "    example = eval_dataset.filter(lambda x: x['id'] == model_id)[0]\n",
    "    formatted_text = formatting_func(example)\n",
    "    \n",
    "    #print(formatted_text)\n",
    "    model_input = tokenizer(formatted_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "    ft_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_tokens = ft_model.generate(**model_input, max_new_tokens=150)[0]\n",
    "        generated_text = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### The job description: Senior Software Engineer - Payroll (San Francisco / Remote (US))\n",
      "About Rippling\n",
      "Rippling is the first way for businesses to manage all of their HR & IT —  payroll, benefits, computers, apps, and more — in one unified workforce platform.\n",
      "By connecting every workforce system to a single source of truth for employee data, businesses can automate all of the manual work they normally need to do to make employee changes. Take onboarding, for example. With Rippling, you can just click a button and set up a new employees' payroll, health insurance, work computer, and third-party apps — like Slack, Zoom, and Office 365 — all within 90 seconds.\n",
      "Based in San Francisco, CA, Rippling has raised $450M from the world’s top investors—including Kleiner Perkins, Founders Fund, and Sequoia—and was named one of America's best startup employers by Forbes (#12 out of 500).\n",
      "About The Role\n",
      "Rippling has 15+ different products and Payroll is the core offering that is used by almost all our customers. The full-service product simplifies each payroll cycle’s effort from days/weeks of data gathering/crunching to just one click. Behind the scenes, our software handles all the hard work - collecting time and attendance data, accounting pre-tax benefits, withholding 7000+ kinds of taxes, paying each employee, paying federal/state/local tax agencies, and filing tax returns for employers.\n",
      "We are looking for talented software engineers at all levels to join a newly formed team to tackle the most challenging part of payroll - tax filings. You will be part of a team of 10+ engineers who build a new system to streamline the complex process involving customers, tax agencies, and internal ops staff. It’s a high impact team where you will learn and grow fast.\n",
      "What You'll Do\n",
      "\n",
      "Work with product managers and other partner teams to understand complex business logic\n",
      "Come up with innovative software solutions to automate and optimize workflows\n",
      "Design and implement scalable and performant backend systems\n",
      "Ship and maintain reliable production systems\n",
      "Provide technical leadership and mentor junior team members\n",
      "\n",
      "Qualifications\n",
      "\n",
      "Solid computer science foundation and strong algo/coding skills\n",
      "Experience building production backend systems\n",
      "Proficient in at least one of Python, Go, JavaScript, Java, etc\n",
      "Good understanding of SQL and NoSQL databases\n",
      "Familiar with cloud platforms such as AWS, GCP\n",
      "5+ years of experience building complex and large-scale backend systems\n",
      "\n",
      "#LI-Remote\n",
      "If you don’t meet all of the requirements listed here, we still encourage you to apply. No job description is perfect, and we might find an even more suitable opportunity that matches your skills and experience.\n",
      "Rippling is an equal opportunity employer. We are committed to building a diverse and inclusive workforce and do not discriminate based on race, religion, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, age, sexual orientation, veteran or military status, or any other legally protected characteristics.  Rippling is committed to providing reasonable accommodations for candidates with disabilities who need assistance during the hiring process. To request a reasonable accommodation, please email accommodations[at]rippling.com.\n",
      "We are committed both to the health of our employees and to promoting a safe and collaborative workplace, and vaccinations are the best way to end the COVID-19 pandemic and to protect our community. In the U.S., where permitted under federal and state law, all offers of employment will be conditioned upon new hires providing proof of vaccination prior to their start date, unless the individual qualifies for an accommodation. For all other locations, vaccinations are strongly encouraged.\n",
      " ### The skills: 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "run_finetune_model(\"15503\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

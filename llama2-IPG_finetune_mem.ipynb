{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXIFTFapAMI"
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuning/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcE4NTeFyRgd"
   },
   "source": [
    "### 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "s6f4z8EYmcJ6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('json', data_files='/home/chuning/Downloads/llama/test/new_train_data.json', split='train')  \n",
    "eval_dataset = load_dataset('json', data_files='/home/chuning/Downloads/llama/test/new_test_data.json', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10696',\n",
       " 'skills': ['programming',\n",
       "  'nltk',\n",
       "  'keras',\n",
       "  'tensorflow',\n",
       "  'gensim',\n",
       "  'xgboost',\n",
       "  'spark',\n",
       "  'presto',\n",
       "  'big data',\n",
       "  'linkedin',\n",
       "  'textblob',\n",
       "  'cloud',\n",
       "  'mxnet',\n",
       "  'red',\n",
       "  'spacy',\n",
       "  'lightgbm',\n",
       "  'deep learning',\n",
       "  'hbase',\n",
       "  'hadoop',\n",
       "  'tableau',\n",
       "  'gcp',\n",
       "  'pytorch',\n",
       "  'ml',\n",
       "  'aws',\n",
       "  'pig',\n",
       "  'nlp',\n",
       "  'machine learning',\n",
       "  'docker',\n",
       "  'automation'],\n",
       " 'not_skills': ['classification',\n",
       "  'apache',\n",
       "  'numpy',\n",
       "  'sql',\n",
       "  'clustering',\n",
       "  'matlab',\n",
       "  'data visualization',\n",
       "  'computer science',\n",
       "  'scikit-learn',\n",
       "  'ai',\n",
       "  'svm',\n",
       "  'natural language processing',\n",
       "  'python',\n",
       "  'gpu',\n",
       "  'prediction',\n",
       "  'java',\n",
       "  'pandas',\n",
       "  'data analysis'],\n",
       " 'text': \"Description\\n\\nSmartNews is a leading mobile app of news aggregation services. It analyzes millions of articles to deliver most engaging information with high quality in near-real time fashion to millions of users around the world. Our AI Foundation team is responsible to research and develop world-class AI algorithms that can be applied at large scale to accomplish our mission. It works on a range of content understanding, user modeling and recommendation problems, which include natural language processing tasks of classification, entity recognition, summarization, computer vision tasks of image/video classification, recommendation tasks of retrieving and ranking for relevance, etc. The team provides important content/user signals and state-of-art recommendation models to the News Ranking/Ads Ranking team to deliver the world's high-quality information to the people who need it.\\nRESPONSIBILITIES\\n\\nSet technical and research directions for AI Foundation or and cross-team machine learning projects and able to lead its implementation\\nThe ability to solve important issues of AI Foundation team from fundamental algorithm development, implementation and optimization to deliver product metrics\\nLead cross-team projects to improve features/models that benefit company OKR\\nIdentify new directions for the team to be industry leading in areas of personalized discovery and content understanding\\nIn this position, you are expected to utilize your industry leading expertise on one or more of the following R&D areas to provide cutting edge solutions or core technologies for SmartNews recommendation systems (Ads, News, etc).\\n\\nGeneral Machine Learning, Deep Learning\\nNatural Language Processing (entity recognition, categorization, text embedding, etc)\\nComputer Vision (object recognition, multimodal content classification, etc)\\nKnowledge Graph\\nRecommendation and ranking algorithms\\nBe great mentor to other machine learning scientists\\n\\n\\n\\n\\n\\n\\nRequirements\\n\\nMinimum Qualifications\\n\\n2+ years of experience in designing and implementing state-of-the-art machine learning algorithms, and applying them to real world problems\\nIndustry leading expertise in certain domain of machine learning techniques, especially in deep learning, natural language processing, recommendation systems, computer visions\\nTrack record of successfully deliver improvement of features/models to production systems with high impact by working across teams and organizations\\nInfluential publications at top industry conferences/journals, well recognized in the industry for the domain of expertise\\nStrong mentors of senior machine learning scientists and able to grow them to the next level\\nGood written and spoken communication skills, can work across functional teams\\nStrong coding skills in multiple programming languages (e.g. Java, C++, Python, Scala)\\n\\nPreferred Qualifications\\n\\nStrong interest in news media and our mission\\nStrong domain expertise in recommendation algorithms\\nM.S or Ph.D in computer science, mathematics, physics or other quantitative fields\\n\\n\\n\\n\\nBenefits\\n\\n\\nEquity included\\n100% medical, dental and vision insurance coverage (100% coverage for dependents)\\nMonthly housing, commute, mobile phone and gym allowances\\n401k matching program\\nFree lunch, snacks, drinks, etc.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting prompts\n",
    "\n",
    "\n",
    "```python\n",
    "def formatting_func(example):\n",
    "    text = f\"### The following is a note by Eevee the Dog: {example['note']}\"\n",
    "    return text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def formatting_func(example):\n",
    "#     text = f\"### The job description: {example['text']}\\n ### The skills: {example['skills']}\"\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Job Description: {example['text']}\\n### Skills: {', '.join(example['skills'])}\\n### Not Skills: {', '.join(example['not_skills'])}\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 2. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-5idQwzvg-"
   },
   "source": [
    "Let's now load Llama 2 13B - `meta-llama/Llama-2-13b-hf` - using 4 or 8-bit quantization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E0Nl5mWL0k2T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m base_model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-13b-hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     bnb_4bit_use_double_quant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     bnb_4bit_compute_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(base_model_id, quantization_config\u001b[39m=\u001b[39;49mbnb_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py:3480\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3472\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3473\u001b[0m     (\n\u001b[1;32m   3474\u001b[0m         model,\n\u001b[1;32m   3475\u001b[0m         missing_keys,\n\u001b[1;32m   3476\u001b[0m         unexpected_keys,\n\u001b[1;32m   3477\u001b[0m         mismatched_keys,\n\u001b[1;32m   3478\u001b[0m         offload_index,\n\u001b[1;32m   3479\u001b[0m         error_msgs,\n\u001b[0;32m-> 3480\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3481\u001b[0m         model,\n\u001b[1;32m   3482\u001b[0m         state_dict,\n\u001b[1;32m   3483\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3484\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3485\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3486\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3487\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3488\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3489\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3490\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3491\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3492\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3493\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3494\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3495\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3496\u001b[0m     )\n\u001b[1;32m   3498\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3499\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py:3870\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3868\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   3869\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fsdp_enabled() \u001b[39mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[0;32m-> 3870\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3871\u001b[0m             model_to_load,\n\u001b[1;32m   3872\u001b[0m             state_dict,\n\u001b[1;32m   3873\u001b[0m             loaded_keys,\n\u001b[1;32m   3874\u001b[0m             start_prefix,\n\u001b[1;32m   3875\u001b[0m             expected_keys,\n\u001b[1;32m   3876\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3877\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3878\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3879\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3880\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3881\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3882\u001b[0m             is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   3883\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3884\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3885\u001b[0m         )\n\u001b[1;32m   3886\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3887\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/modeling_utils.py:751\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    748\u001b[0m             fp16_statistics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    750\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m param_name:\n\u001b[0;32m--> 751\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[1;32m    752\u001b[0m                 model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam, fp16_statistics\u001b[39m=\u001b[39;49mfp16_statistics\n\u001b[1;32m    753\u001b[0m             )\n\u001b[1;32m    755\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py:98\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, fp16_statistics)\u001b[0m\n\u001b[1;32m     96\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mInt8Params(new_value, requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     97\u001b[0m \u001b[39melif\u001b[39;00m is_4bit:\n\u001b[0;32m---> 98\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mParams4bit(new_value, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    100\u001b[0m module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m new_value\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m fp16_statistics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:191\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m (device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:168\u001b[0m, in \u001b[0;36mParams4bit.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m, device):\n\u001b[0;32m--> 168\u001b[0m     w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mhalf()\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    169\u001b[0m     w_4bit, quant_state \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mquantize_4bit(w, blocksize\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize, compress_statistics\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompress_statistics, quant_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_type)\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m w_4bit\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# import torch\n",
    "\n",
    "\n",
    "# base_model_id = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "# # 使用8-bit量化配置\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=False, \n",
    "#     bnb_4bit_compute_dtype=torch.float32  \n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config).to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "Set up the tokenizer. Add padding on the left as it [makes training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa).\n",
    "\n",
    "\n",
    "For `model_max_length`, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haSUDD9HyRgf"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_length = 1024\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     # Tokenize the input\n",
    "#     tokenized_text = tokenizer(\n",
    "#         formatting_func(prompt),\n",
    "#         #prompt['text'],\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "#     # Initialize labels with 0 (neither skill nor not-skill)\n",
    "#     labels = [0] * len(tokenized_text['input_ids'])\n",
    "\n",
    "#     # Update labels for skills (1) and not-skills (2)\n",
    "#     for i, token_id in enumerate(tokenized_text['input_ids']):\n",
    "#         token = tokenizer.decode([token_id])\n",
    "#         if any(skill in prompt['text'] for skill in prompt['skills']):\n",
    "#             labels[i] = 1\n",
    "#         elif any(not_skill in prompt['text'] for not_skill in prompt['not_skills']):\n",
    "#             labels[i] = 2\n",
    "\n",
    "#     tokenized_text[\"labels\"] = labels\n",
    "#     return tokenized_text\n",
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     # Tokenize the input text\n",
    "#     tokenized_text = tokenizer(\n",
    "#         formatting_func(prompt),\n",
    "#         #prompt['text'],\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "#     # Initialize labels with 0 (neither skill nor not-skill)\n",
    "#     labels = [-100] * len(tokenized_text['input_ids'])\n",
    "\n",
    "#     # Process each token to determine if it is a part of skills or not-skills\n",
    "#     for i, token_id in enumerate(tokenized_text['input_ids']):\n",
    "#         token = tokenizer.decode([token_id], clean_up_tokenization_spaces=False)\n",
    "#         if token.strip() and any(token in skill for skill in prompt['skills']):\n",
    "#             labels[i] = 1\n",
    "#         elif token.strip() and any(token in not_skill for not_skill in prompt['not_skills']):\n",
    "#             labels[i] = 2\n",
    "\n",
    "#     tokenized_text[\"labels\"] = labels\n",
    "#     return tokenized_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     # Tokenize the input text\n",
    "#     tokenized_text = tokenizer(\n",
    "#         #prompt['text'],\n",
    "#         formatting_func(prompt),\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "\n",
    "#     # Initialize labels with -100 (ignore index)\n",
    "#     labels = [-100] * len(tokenized_text['input_ids'])\n",
    "\n",
    "#     # Function to find indices of a sub-list in a list\n",
    "#     def find_sub_list(sub_list, main_list):\n",
    "#         results = []\n",
    "#         sll = len(sub_list)\n",
    "#         for ind in (i for i, e in enumerate(main_list) if e == sub_list[0]):\n",
    "#             if main_list[ind:ind+sll] == sub_list:\n",
    "#                 results.append((ind, ind+sll-1))\n",
    "#         return results\n",
    "\n",
    "#     # Assign labels for skills\n",
    "#     for skill in prompt['skills']:\n",
    "#         skill_tokens = tokenizer.encode(skill, add_special_tokens=False)\n",
    "#         for start, end in find_sub_list(skill_tokens, tokenized_text['input_ids']):\n",
    "#             labels[start:end+1] = [1] * (end - start + 1)\n",
    "\n",
    "#     # Assign labels for not-skills\n",
    "#     for not_skill in prompt['not_skills']:\n",
    "#         not_skill_tokens = tokenizer.encode(not_skill, add_special_tokens=False)\n",
    "#         for start, end in find_sub_list(not_skill_tokens, tokenized_text['input_ids']):\n",
    "#             labels[start:end+1] = [2] * (end - start + 1)\n",
    "\n",
    "#     tokenized_text[\"labels\"] = labels\n",
    "#     return tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     # Tokenize the job description text\n",
    "#     formatted_text = formatting_func(prompt)\n",
    "#     tokenized_text = tokenizer(\n",
    "#         formatted_text,\n",
    "#         #prompt['text']\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding='max_length', \n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "\n",
    "#     # Start labels with -100 to ignore these tokens during loss computation\n",
    "#     labels = [-100] * len(tokenized_text['input_ids'])\n",
    "\n",
    "#     # Tokenize and label skills\n",
    "#     for skill in prompt['skills']:\n",
    "#         skill_tokens = tokenizer.encode(skill, add_special_tokens=False)\n",
    "#         labels.extend([1] * len(skill_tokens))  # Label for skill\n",
    "#         tokenized_text['input_ids'] = torch.cat([tokenized_text['input_ids'], \n",
    "#                                                  torch.tensor([skill_tokens], dtype=torch.long)], dim=1)\n",
    "\n",
    "#     # Tokenize and label not-skills\n",
    "#     for not_skill in prompt['not_skills']:\n",
    "#         not_skill_tokens = tokenizer.encode(not_skill, add_special_tokens=False)\n",
    "#         labels.extend([2] * len(not_skill_tokens))  # Label for not-skill\n",
    "#         tokenized_text['input_ids'] = torch.cat([tokenized_text['input_ids'], \n",
    "#                                                  torch.tensor([not_skill_tokens], dtype=torch.long)], dim=1)\n",
    "\n",
    "#     # Add padding manually if necessary\n",
    "#     if len(tokenized_text['input_ids'][0]) < max_length:\n",
    "#         padding_length = max_length - len(tokenized_text['input_ids'][0])\n",
    "#         tokenized_text['input_ids'] = torch.cat([tokenized_text['input_ids'], \n",
    "#                                                  torch.tensor([[tokenizer.pad_token_id] * padding_length], dtype=torch.long)], dim=1)\n",
    "#         labels.extend([-100] * padding_length)  # Pad labels with -100\n",
    "\n",
    "#     # tokenized_text['labels'] = torch.tensor([labels], dtype=torch.long)\n",
    "#     tokenized_text['labels'] = torch.tensor([labels], dtype=torch.long)\n",
    "#     #tokenized_text[\"labels\"] = tokenized_text[\"input_ids\"].copy()\n",
    "#     return tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     # Tokenize the job description text\n",
    "#     formatted_text = formatting_func(prompt)\n",
    "#     tokenized_text = tokenizer(\n",
    "#         formatted_text,\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding='max_length', \n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "\n",
    "#     # Initialize labels with -100 to ignore these tokens during loss computation\n",
    "#     labels = [-100] * max_length\n",
    "\n",
    "#     # Assign labels for skills and not-skills\n",
    "#     all_skills = prompt['skills'] + prompt['not_skills']\n",
    "#     for skill in all_skills:\n",
    "#         skill_label = 1 if skill in prompt['skills'] else 2\n",
    "#         skill_tokens = tokenizer.encode(skill, add_special_tokens=False)\n",
    "#         for i, token_id in enumerate(tokenized_text['input_ids'][0]):\n",
    "#             if token_id in skill_tokens:\n",
    "#                 labels[i] = skill_label\n",
    "\n",
    "#     # Convert labels to a tensor\n",
    "#     tokenized_text['labels'] = torch.tensor([labels])\n",
    "\n",
    "#     return tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     # Tokenize the job description text\n",
    "#     formatted_text = formatting_func(prompt)\n",
    "#     tokenized_text = tokenizer(\n",
    "#         formatted_text,\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding='max_length', \n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "\n",
    "#     # Initialize labels with -100 to ignore these tokens during loss computation\n",
    "#     labels = [-100] * max_length\n",
    "\n",
    "#     # Process each skill and not-skill\n",
    "#     for skill in prompt['skills']:\n",
    "#         labels = label_tokens(tokenized_text, skill, 1, labels)\n",
    "#     for not_skill in prompt['not_skills']:\n",
    "#         labels = label_tokens(tokenized_text, not_skill, 2, labels)\n",
    "\n",
    "#     # Convert labels to a tensor\n",
    "#     tokenized_text['labels'] = torch.tensor([labels])\n",
    "#     return tokenized_text\n",
    "\n",
    "# def label_tokens(tokenized_text, phrase, label, labels):\n",
    "#     # Tokenize the phrase without special tokens\n",
    "#     phrase_tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "#     # Find start index of phrase tokens in input_ids\n",
    "#     for i in range(len(tokenized_text['input_ids'][0]) - len(phrase_tokens) + 1):\n",
    "#         if tokenized_text['input_ids'][0][i:i+len(phrase_tokens)].tolist() == phrase_tokens:\n",
    "#             # Assign label to each token in the phrase\n",
    "#             for j in range(i, i+len(phrase_tokens)):\n",
    "#                 labels[j] = label\n",
    "#     return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     # Tokenize the job description text\n",
    "#     formatted_text = formatting_func(prompt)\n",
    "#     tokenized_text = tokenizer(\n",
    "#         formatted_text,\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding='max_length',\n",
    "#         return_tensors=\"pt\"  # Ensure this is set\n",
    "#     )\n",
    "\n",
    "#     # Initialize labels with -100 for each token\n",
    "#     labels = [-100] * max_length\n",
    "\n",
    "#     # Assign labels for skills and not-skills\n",
    "#     all_skills = prompt['skills'] + prompt['not_skills']\n",
    "#     for skill in all_skills:\n",
    "#         skill_label = 1 if skill in prompt['skills'] else 2\n",
    "#         skill_tokens = tokenizer.encode(skill, add_special_tokens=False)\n",
    "#         for i, token_id in enumerate(tokenized_text['input_ids'][0]):\n",
    "#             if token_id in skill_tokens:\n",
    "#                 labels[i] = skill_label\n",
    "\n",
    "#     # Convert labels to a tensor\n",
    "#     tokenized_text['labels'] = torch.tensor(labels).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "#     return tokenized_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "id: type=<class 'str'>, shape=N/A\n",
      "skills: type=<class 'list'>, shape=N/A\n",
      "not_skills: type=<class 'list'>, shape=N/A\n",
      "text: type=<class 'str'>, shape=N/A\n",
      "input_ids: type=<class 'list'>, shape=N/A\n",
      "attention_mask: type=<class 'list'>, shape=N/A\n",
      "labels: type=<class 'list'>, shape=N/A\n",
      "Batch 1:\n",
      "id: type=<class 'str'>, shape=N/A\n",
      "skills: type=<class 'list'>, shape=N/A\n",
      "not_skills: type=<class 'list'>, shape=N/A\n",
      "text: type=<class 'str'>, shape=N/A\n",
      "input_ids: type=<class 'list'>, shape=N/A\n",
      "attention_mask: type=<class 'list'>, shape=N/A\n",
      "labels: type=<class 'list'>, shape=N/A\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tokenized_train_dataset):\n",
    "    print(f\"Batch {i}:\")\n",
    "    if isinstance(batch, dict):\n",
    "        for key in batch:\n",
    "            print(f\"{key}: type={type(batch[key])}, shape={getattr(batch[key], 'shape', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"Unexpected format: type={type(batch)}\")\n",
    "    if i >= 1:  # Check the first couple of batches\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '10696',\n",
       " 'skills': ['programming',\n",
       "  'nltk',\n",
       "  'keras',\n",
       "  'tensorflow',\n",
       "  'gensim',\n",
       "  'xgboost',\n",
       "  'spark',\n",
       "  'presto',\n",
       "  'big data',\n",
       "  'linkedin',\n",
       "  'textblob',\n",
       "  'cloud',\n",
       "  'mxnet',\n",
       "  'red',\n",
       "  'spacy',\n",
       "  'lightgbm',\n",
       "  'deep learning',\n",
       "  'hbase',\n",
       "  'hadoop',\n",
       "  'tableau',\n",
       "  'gcp',\n",
       "  'pytorch',\n",
       "  'ml',\n",
       "  'aws',\n",
       "  'pig',\n",
       "  'nlp',\n",
       "  'machine learning',\n",
       "  'docker',\n",
       "  'automation'],\n",
       " 'not_skills': ['classification',\n",
       "  'apache',\n",
       "  'numpy',\n",
       "  'sql',\n",
       "  'clustering',\n",
       "  'matlab',\n",
       "  'data visualization',\n",
       "  'computer science',\n",
       "  'scikit-learn',\n",
       "  'ai',\n",
       "  'svm',\n",
       "  'natural language processing',\n",
       "  'python',\n",
       "  'gpu',\n",
       "  'prediction',\n",
       "  'java',\n",
       "  'pandas',\n",
       "  'data analysis'],\n",
       " 'text': \"Description\\n\\nSmartNews is a leading mobile app of news aggregation services. It analyzes millions of articles to deliver most engaging information with high quality in near-real time fashion to millions of users around the world. Our AI Foundation team is responsible to research and develop world-class AI algorithms that can be applied at large scale to accomplish our mission. It works on a range of content understanding, user modeling and recommendation problems, which include natural language processing tasks of classification, entity recognition, summarization, computer vision tasks of image/video classification, recommendation tasks of retrieving and ranking for relevance, etc. The team provides important content/user signals and state-of-art recommendation models to the News Ranking/Ads Ranking team to deliver the world's high-quality information to the people who need it.\\nRESPONSIBILITIES\\n\\nSet technical and research directions for AI Foundation or and cross-team machine learning projects and able to lead its implementation\\nThe ability to solve important issues of AI Foundation team from fundamental algorithm development, implementation and optimization to deliver product metrics\\nLead cross-team projects to improve features/models that benefit company OKR\\nIdentify new directions for the team to be industry leading in areas of personalized discovery and content understanding\\nIn this position, you are expected to utilize your industry leading expertise on one or more of the following R&D areas to provide cutting edge solutions or core technologies for SmartNews recommendation systems (Ads, News, etc).\\n\\nGeneral Machine Learning, Deep Learning\\nNatural Language Processing (entity recognition, categorization, text embedding, etc)\\nComputer Vision (object recognition, multimodal content classification, etc)\\nKnowledge Graph\\nRecommendation and ranking algorithms\\nBe great mentor to other machine learning scientists\\n\\n\\n\\n\\n\\n\\nRequirements\\n\\nMinimum Qualifications\\n\\n2+ years of experience in designing and implementing state-of-the-art machine learning algorithms, and applying them to real world problems\\nIndustry leading expertise in certain domain of machine learning techniques, especially in deep learning, natural language processing, recommendation systems, computer visions\\nTrack record of successfully deliver improvement of features/models to production systems with high impact by working across teams and organizations\\nInfluential publications at top industry conferences/journals, well recognized in the industry for the domain of expertise\\nStrong mentors of senior machine learning scientists and able to grow them to the next level\\nGood written and spoken communication skills, can work across functional teams\\nStrong coding skills in multiple programming languages (e.g. Java, C++, Python, Scala)\\n\\nPreferred Qualifications\\n\\nStrong interest in news media and our mission\\nStrong domain expertise in recommendation algorithms\\nM.S or Ph.D in computer science, mathematics, physics or other quantitative fields\\n\\n\\n\\n\\nBenefits\\n\\n\\nEquity included\\n100% medical, dental and vision insurance coverage (100% coverage for dependents)\\nMonthly housing, commute, mobile phone and gym allowances\\n401k matching program\\nFree lunch, snacks, drinks, etc.\",\n",
       " 'input_ids': [2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  835,\n",
       "  17163,\n",
       "  12953,\n",
       "  29901,\n",
       "  12953,\n",
       "  13,\n",
       "  13,\n",
       "  12636,\n",
       "  442,\n",
       "  29328,\n",
       "  338,\n",
       "  263,\n",
       "  8236,\n",
       "  10426,\n",
       "  623,\n",
       "  310,\n",
       "  9763,\n",
       "  11404,\n",
       "  362,\n",
       "  5786,\n",
       "  29889,\n",
       "  739,\n",
       "  16455,\n",
       "  10947,\n",
       "  14746,\n",
       "  310,\n",
       "  7456,\n",
       "  304,\n",
       "  12021,\n",
       "  1556,\n",
       "  3033,\n",
       "  6751,\n",
       "  2472,\n",
       "  411,\n",
       "  1880,\n",
       "  11029,\n",
       "  297,\n",
       "  2978,\n",
       "  29899,\n",
       "  6370,\n",
       "  931,\n",
       "  13460,\n",
       "  304,\n",
       "  14746,\n",
       "  310,\n",
       "  4160,\n",
       "  2820,\n",
       "  278,\n",
       "  3186,\n",
       "  29889,\n",
       "  8680,\n",
       "  319,\n",
       "  29902,\n",
       "  10606,\n",
       "  3815,\n",
       "  338,\n",
       "  14040,\n",
       "  304,\n",
       "  5925,\n",
       "  322,\n",
       "  2693,\n",
       "  3186,\n",
       "  29899,\n",
       "  1990,\n",
       "  319,\n",
       "  29902,\n",
       "  14009,\n",
       "  393,\n",
       "  508,\n",
       "  367,\n",
       "  7436,\n",
       "  472,\n",
       "  2919,\n",
       "  6287,\n",
       "  304,\n",
       "  12709,\n",
       "  1749,\n",
       "  10655,\n",
       "  29889,\n",
       "  739,\n",
       "  1736,\n",
       "  373,\n",
       "  263,\n",
       "  3464,\n",
       "  310,\n",
       "  2793,\n",
       "  8004,\n",
       "  29892,\n",
       "  1404,\n",
       "  1904,\n",
       "  292,\n",
       "  322,\n",
       "  29303,\n",
       "  4828,\n",
       "  29892,\n",
       "  607,\n",
       "  3160,\n",
       "  5613,\n",
       "  4086,\n",
       "  9068,\n",
       "  9595,\n",
       "  310,\n",
       "  12965,\n",
       "  29892,\n",
       "  7855,\n",
       "  19679,\n",
       "  29892,\n",
       "  19138,\n",
       "  2133,\n",
       "  29892,\n",
       "  6601,\n",
       "  18551,\n",
       "  9595,\n",
       "  310,\n",
       "  1967,\n",
       "  29914,\n",
       "  9641,\n",
       "  12965,\n",
       "  29892,\n",
       "  29303,\n",
       "  9595,\n",
       "  310,\n",
       "  5663,\n",
       "  15387,\n",
       "  322,\n",
       "  24034,\n",
       "  363,\n",
       "  29527,\n",
       "  749,\n",
       "  29892,\n",
       "  2992,\n",
       "  29889,\n",
       "  450,\n",
       "  3815,\n",
       "  8128,\n",
       "  4100,\n",
       "  2793,\n",
       "  29914,\n",
       "  1792,\n",
       "  18470,\n",
       "  322,\n",
       "  2106,\n",
       "  29899,\n",
       "  974,\n",
       "  29899,\n",
       "  442,\n",
       "  29303,\n",
       "  4733,\n",
       "  304,\n",
       "  278,\n",
       "  10130,\n",
       "  22125,\n",
       "  292,\n",
       "  29914,\n",
       "  3253,\n",
       "  29879,\n",
       "  22125,\n",
       "  292,\n",
       "  3815,\n",
       "  304,\n",
       "  12021,\n",
       "  278,\n",
       "  3186,\n",
       "  29915,\n",
       "  29879,\n",
       "  1880,\n",
       "  29899,\n",
       "  29567,\n",
       "  2472,\n",
       "  304,\n",
       "  278,\n",
       "  2305,\n",
       "  1058,\n",
       "  817,\n",
       "  372,\n",
       "  29889,\n",
       "  13,\n",
       "  1525,\n",
       "  5550,\n",
       "  1164,\n",
       "  5425,\n",
       "  29933,\n",
       "  6227,\n",
       "  1806,\n",
       "  29059,\n",
       "  13,\n",
       "  13,\n",
       "  2697,\n",
       "  16905,\n",
       "  322,\n",
       "  5925,\n",
       "  18112,\n",
       "  363,\n",
       "  319,\n",
       "  29902,\n",
       "  10606,\n",
       "  470,\n",
       "  322,\n",
       "  4891,\n",
       "  29899,\n",
       "  14318,\n",
       "  4933,\n",
       "  6509,\n",
       "  9279,\n",
       "  322,\n",
       "  2221,\n",
       "  304,\n",
       "  3275,\n",
       "  967,\n",
       "  5314,\n",
       "  13,\n",
       "  1576,\n",
       "  11509,\n",
       "  304,\n",
       "  4505,\n",
       "  4100,\n",
       "  5626,\n",
       "  310,\n",
       "  319,\n",
       "  29902,\n",
       "  10606,\n",
       "  3815,\n",
       "  515,\n",
       "  15281,\n",
       "  5687,\n",
       "  5849,\n",
       "  29892,\n",
       "  5314,\n",
       "  322,\n",
       "  13883,\n",
       "  304,\n",
       "  12021,\n",
       "  3234,\n",
       "  21556,\n",
       "  13,\n",
       "  29931,\n",
       "  1479,\n",
       "  4891,\n",
       "  29899,\n",
       "  14318,\n",
       "  9279,\n",
       "  304,\n",
       "  11157,\n",
       "  5680,\n",
       "  29914,\n",
       "  9794,\n",
       "  393,\n",
       "  14169,\n",
       "  5001,\n",
       "  9280,\n",
       "  29934,\n",
       "  13,\n",
       "  7648,\n",
       "  1598,\n",
       "  716,\n",
       "  18112,\n",
       "  363,\n",
       "  278,\n",
       "  3815,\n",
       "  304,\n",
       "  367,\n",
       "  13661,\n",
       "  8236,\n",
       "  297,\n",
       "  10161,\n",
       "  310,\n",
       "  7333,\n",
       "  1891,\n",
       "  20699,\n",
       "  322,\n",
       "  2793,\n",
       "  8004,\n",
       "  13,\n",
       "  797,\n",
       "  445,\n",
       "  2602,\n",
       "  29892,\n",
       "  366,\n",
       "  526,\n",
       "  3806,\n",
       "  304,\n",
       "  3667,\n",
       "  675,\n",
       "  596,\n",
       "  13661,\n",
       "  8236,\n",
       "  17924,\n",
       "  895,\n",
       "  373,\n",
       "  697,\n",
       "  470,\n",
       "  901,\n",
       "  310,\n",
       "  278,\n",
       "  1494,\n",
       "  390,\n",
       "  29987,\n",
       "  29928,\n",
       "  10161,\n",
       "  304,\n",
       "  3867,\n",
       "  28967,\n",
       "  7636,\n",
       "  6851,\n",
       "  470,\n",
       "  7136,\n",
       "  5722,\n",
       "  11763,\n",
       "  363,\n",
       "  4116,\n",
       "  442,\n",
       "  29328,\n",
       "  29303,\n",
       "  6757,\n",
       "  313,\n",
       "  3253,\n",
       "  29879,\n",
       "  29892,\n",
       "  10130,\n",
       "  29892,\n",
       "  2992,\n",
       "  467,\n",
       "  13,\n",
       "  13,\n",
       "  15263,\n",
       "  6189,\n",
       "  29257,\n",
       "  29892,\n",
       "  21784,\n",
       "  29257,\n",
       "  13,\n",
       "  29940,\n",
       "  18771,\n",
       "  17088,\n",
       "  10554,\n",
       "  292,\n",
       "  313,\n",
       "  10041,\n",
       "  19679,\n",
       "  29892,\n",
       "  11608,\n",
       "  2133,\n",
       "  29892,\n",
       "  1426,\n",
       "  23655,\n",
       "  29892,\n",
       "  2992,\n",
       "  29897,\n",
       "  13,\n",
       "  20606,\n",
       "  261,\n",
       "  478,\n",
       "  2459,\n",
       "  313,\n",
       "  3318,\n",
       "  19679,\n",
       "  29892,\n",
       "  1773,\n",
       "  326,\n",
       "  397,\n",
       "  284,\n",
       "  2793,\n",
       "  12965,\n",
       "  29892,\n",
       "  2992,\n",
       "  29897,\n",
       "  13,\n",
       "  29968,\n",
       "  3707,\n",
       "  5485,\n",
       "  12367,\n",
       "  13,\n",
       "  1123,\n",
       "  2055,\n",
       "  355,\n",
       "  362,\n",
       "  322,\n",
       "  24034,\n",
       "  14009,\n",
       "  13,\n",
       "  3629,\n",
       "  2107,\n",
       "  6042,\n",
       "  272,\n",
       "  304,\n",
       "  916,\n",
       "  4933,\n",
       "  6509,\n",
       "  9638,\n",
       "  2879,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  1123,\n",
       "  1548,\n",
       "  1860,\n",
       "  13,\n",
       "  13,\n",
       "  8140,\n",
       "  12539,\n",
       "  15146,\n",
       "  8232,\n",
       "  13,\n",
       "  13,\n",
       "  29906,\n",
       "  29974,\n",
       "  2440,\n",
       "  310,\n",
       "  7271,\n",
       "  297,\n",
       "  2874,\n",
       "  292,\n",
       "  322,\n",
       "  16049,\n",
       "  2106,\n",
       "  29899,\n",
       "  974,\n",
       "  29899,\n",
       "  1552,\n",
       "  29899,\n",
       "  442,\n",
       "  4933,\n",
       "  6509,\n",
       "  14009,\n",
       "  29892,\n",
       "  322,\n",
       "  15399,\n",
       "  963,\n",
       "  304,\n",
       "  1855,\n",
       "  3186,\n",
       "  4828,\n",
       "  13,\n",
       "  2568,\n",
       "  504,\n",
       "  719,\n",
       "  8236,\n",
       "  17924,\n",
       "  895,\n",
       "  297,\n",
       "  3058,\n",
       "  5354,\n",
       "  310,\n",
       "  4933,\n",
       "  6509,\n",
       "  13698,\n",
       "  29892,\n",
       "  7148,\n",
       "  297,\n",
       "  6483,\n",
       "  6509,\n",
       "  29892,\n",
       "  5613,\n",
       "  4086,\n",
       "  9068,\n",
       "  29892,\n",
       "  29303,\n",
       "  6757,\n",
       "  29892,\n",
       "  6601,\n",
       "  1998,\n",
       "  1080,\n",
       "  13,\n",
       "  17936,\n",
       "  2407,\n",
       "  310,\n",
       "  8472,\n",
       "  12021,\n",
       "  20414,\n",
       "  310,\n",
       "  5680,\n",
       "  29914,\n",
       "  9794,\n",
       "  304,\n",
       "  5802,\n",
       "  6757,\n",
       "  411,\n",
       "  1880,\n",
       "  10879,\n",
       "  491,\n",
       "  1985,\n",
       "  4822,\n",
       "  10907,\n",
       "  322,\n",
       "  25700,\n",
       "  13,\n",
       "  15403,\n",
       "  29884,\n",
       "  2556,\n",
       "  25964,\n",
       "  472,\n",
       "  2246,\n",
       "  13661,\n",
       "  378,\n",
       "  10662,\n",
       "  29914,\n",
       "  29926,\n",
       "  2905,\n",
       "  1338,\n",
       "  29892,\n",
       "  1532,\n",
       "  14831,\n",
       "  297,\n",
       "  278,\n",
       "  13661,\n",
       "  363,\n",
       "  278,\n",
       "  5354,\n",
       "  310,\n",
       "  17924,\n",
       "  895,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  6042,\n",
       "  943,\n",
       "  310,\n",
       "  16336,\n",
       "  4933,\n",
       "  6509,\n",
       "  9638,\n",
       "  2879,\n",
       "  322,\n",
       "  2221,\n",
       "  304,\n",
       "  6548,\n",
       "  963,\n",
       "  304,\n",
       "  278,\n",
       "  2446,\n",
       "  3233,\n",
       "  13,\n",
       "  18420,\n",
       "  3971,\n",
       "  322,\n",
       "  19182,\n",
       "  12084,\n",
       "  25078,\n",
       "  29892,\n",
       "  508,\n",
       "  664,\n",
       "  4822,\n",
       "  13303,\n",
       "  10907,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  14137,\n",
       "  25078,\n",
       "  297,\n",
       "  2999,\n",
       "  8720,\n",
       "  10276,\n",
       "  313,\n",
       "  29872,\n",
       "  29889,\n",
       "  29887,\n",
       "  29889,\n",
       "  3355,\n",
       "  29892,\n",
       "  315,\n",
       "  10024,\n",
       "  5132,\n",
       "  29892,\n",
       "  24419,\n",
       "  29897,\n",
       "  13,\n",
       "  13,\n",
       "  6572,\n",
       "  14373,\n",
       "  15146,\n",
       "  8232,\n",
       "  13,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  4066,\n",
       "  297,\n",
       "  9763,\n",
       "  5745,\n",
       "  322,\n",
       "  1749,\n",
       "  10655,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  5354,\n",
       "  17924,\n",
       "  895,\n",
       "  297,\n",
       "  29303,\n",
       "  14009,\n",
       "  13,\n",
       "  29924,\n",
       "  29889,\n",
       "  29903,\n",
       "  470,\n",
       "  1963,\n",
       "  29889,\n",
       "  29928,\n",
       "  297,\n",
       "  6601,\n",
       "  10466,\n",
       "  29892,\n",
       "  23964,\n",
       "  29892,\n",
       "  17558,\n",
       "  470,\n",
       "  916,\n",
       "  4323,\n",
       "  23378,\n",
       "  4235,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  20841,\n",
       "  1389,\n",
       "  1169,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  6108,\n",
       "  537,\n",
       "  5134,\n",
       "  13,\n",
       "  29896,\n",
       "  29900,\n",
       "  29900,\n",
       "  29995,\n",
       "  16083,\n",
       "  29892,\n",
       "  12042,\n",
       "  284,\n",
       "  322,\n",
       "  18551,\n",
       "  1663,\n",
       "  18541,\n",
       "  23746,\n",
       "  313,\n",
       "  29896,\n",
       "  29900,\n",
       "  29900,\n",
       "  29995,\n",
       "  23746,\n",
       "  363,\n",
       "  8839,\n",
       "  1237,\n",
       "  29897,\n",
       "  13,\n",
       "  13953,\n",
       "  368,\n",
       "  27261,\n",
       "  29892,\n",
       "  844,\n",
       "  1082,\n",
       "  29892,\n",
       "  10426,\n",
       "  9008,\n",
       "  322,\n",
       "  330,\n",
       "  962,\n",
       "  2758,\n",
       "  2925,\n",
       "  13,\n",
       "  29946,\n",
       "  29900,\n",
       "  29896,\n",
       "  29895,\n",
       "  9686,\n",
       "  1824,\n",
       "  13,\n",
       "  20475,\n",
       "  301,\n",
       "  3322,\n",
       "  29892,\n",
       "  5807,\n",
       "  26514,\n",
       "  29892,\n",
       "  13748,\n",
       "  29879,\n",
       "  29892,\n",
       "  2992,\n",
       "  29889,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  4971,\n",
       "  6090,\n",
       "  29901,\n",
       "  8720,\n",
       "  29892,\n",
       "  302,\n",
       "  1896,\n",
       "  29895,\n",
       "  29892,\n",
       "  13023,\n",
       "  294,\n",
       "  29892,\n",
       "  26110,\n",
       "  29892,\n",
       "  26943,\n",
       "  326,\n",
       "  29892,\n",
       "  921,\n",
       "  29887,\n",
       "  17079,\n",
       "  29892,\n",
       "  16267,\n",
       "  29892,\n",
       "  544,\n",
       "  4778,\n",
       "  29892,\n",
       "  4802,\n",
       "  848,\n",
       "  29892,\n",
       "  9024,\n",
       "  262,\n",
       "  29892,\n",
       "  1426,\n",
       "  10054,\n",
       "  29892,\n",
       "  9570,\n",
       "  29892,\n",
       "  286,\n",
       "  29916,\n",
       "  1212,\n",
       "  29892,\n",
       "  2654,\n",
       "  29892,\n",
       "  805,\n",
       "  4135,\n",
       "  29892,\n",
       "  3578,\n",
       "  29887,\n",
       "  5838,\n",
       "  29892,\n",
       "  6483,\n",
       "  6509,\n",
       "  29892,\n",
       "  298,\n",
       "  3188,\n",
       "  29892,\n",
       "  750,\n",
       "  26793,\n",
       "  29892,\n",
       "  1591,\n",
       "  585,\n",
       "  29892,\n",
       "  330,\n",
       "  6814,\n",
       "  29892,\n",
       "  282,\n",
       "  3637,\n",
       "  25350,\n",
       "  29892,\n",
       "  286,\n",
       "  29880,\n",
       "  29892,\n",
       "  25879,\n",
       "  29892,\n",
       "  282,\n",
       "  335,\n",
       "  29892,\n",
       "  302,\n",
       "  22833,\n",
       "  29892,\n",
       "  4933,\n",
       "  6509,\n",
       "  29892,\n",
       "  10346,\n",
       "  29892,\n",
       "  3345,\n",
       "  362,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  2216,\n",
       "  4971,\n",
       "  6090,\n",
       "  29901,\n",
       "  12965,\n",
       "  29892,\n",
       "  12641,\n",
       "  29892,\n",
       "  12655,\n",
       "  29892,\n",
       "  4576,\n",
       "  29892,\n",
       "  16993,\n",
       "  3241,\n",
       "  29892,\n",
       "  1775,\n",
       "  8205,\n",
       "  29892,\n",
       "  848,\n",
       "  7604,\n",
       "  2133,\n",
       "  29892,\n",
       "  6601,\n",
       "  10466,\n",
       "  29892,\n",
       "  4560,\n",
       "  7354,\n",
       "  29899,\n",
       "  19668,\n",
       "  ...],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'labels': [2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  835,\n",
       "  17163,\n",
       "  12953,\n",
       "  29901,\n",
       "  12953,\n",
       "  13,\n",
       "  13,\n",
       "  12636,\n",
       "  442,\n",
       "  29328,\n",
       "  338,\n",
       "  263,\n",
       "  8236,\n",
       "  10426,\n",
       "  623,\n",
       "  310,\n",
       "  9763,\n",
       "  11404,\n",
       "  362,\n",
       "  5786,\n",
       "  29889,\n",
       "  739,\n",
       "  16455,\n",
       "  10947,\n",
       "  14746,\n",
       "  310,\n",
       "  7456,\n",
       "  304,\n",
       "  12021,\n",
       "  1556,\n",
       "  3033,\n",
       "  6751,\n",
       "  2472,\n",
       "  411,\n",
       "  1880,\n",
       "  11029,\n",
       "  297,\n",
       "  2978,\n",
       "  29899,\n",
       "  6370,\n",
       "  931,\n",
       "  13460,\n",
       "  304,\n",
       "  14746,\n",
       "  310,\n",
       "  4160,\n",
       "  2820,\n",
       "  278,\n",
       "  3186,\n",
       "  29889,\n",
       "  8680,\n",
       "  319,\n",
       "  29902,\n",
       "  10606,\n",
       "  3815,\n",
       "  338,\n",
       "  14040,\n",
       "  304,\n",
       "  5925,\n",
       "  322,\n",
       "  2693,\n",
       "  3186,\n",
       "  29899,\n",
       "  1990,\n",
       "  319,\n",
       "  29902,\n",
       "  14009,\n",
       "  393,\n",
       "  508,\n",
       "  367,\n",
       "  7436,\n",
       "  472,\n",
       "  2919,\n",
       "  6287,\n",
       "  304,\n",
       "  12709,\n",
       "  1749,\n",
       "  10655,\n",
       "  29889,\n",
       "  739,\n",
       "  1736,\n",
       "  373,\n",
       "  263,\n",
       "  3464,\n",
       "  310,\n",
       "  2793,\n",
       "  8004,\n",
       "  29892,\n",
       "  1404,\n",
       "  1904,\n",
       "  292,\n",
       "  322,\n",
       "  29303,\n",
       "  4828,\n",
       "  29892,\n",
       "  607,\n",
       "  3160,\n",
       "  5613,\n",
       "  4086,\n",
       "  9068,\n",
       "  9595,\n",
       "  310,\n",
       "  12965,\n",
       "  29892,\n",
       "  7855,\n",
       "  19679,\n",
       "  29892,\n",
       "  19138,\n",
       "  2133,\n",
       "  29892,\n",
       "  6601,\n",
       "  18551,\n",
       "  9595,\n",
       "  310,\n",
       "  1967,\n",
       "  29914,\n",
       "  9641,\n",
       "  12965,\n",
       "  29892,\n",
       "  29303,\n",
       "  9595,\n",
       "  310,\n",
       "  5663,\n",
       "  15387,\n",
       "  322,\n",
       "  24034,\n",
       "  363,\n",
       "  29527,\n",
       "  749,\n",
       "  29892,\n",
       "  2992,\n",
       "  29889,\n",
       "  450,\n",
       "  3815,\n",
       "  8128,\n",
       "  4100,\n",
       "  2793,\n",
       "  29914,\n",
       "  1792,\n",
       "  18470,\n",
       "  322,\n",
       "  2106,\n",
       "  29899,\n",
       "  974,\n",
       "  29899,\n",
       "  442,\n",
       "  29303,\n",
       "  4733,\n",
       "  304,\n",
       "  278,\n",
       "  10130,\n",
       "  22125,\n",
       "  292,\n",
       "  29914,\n",
       "  3253,\n",
       "  29879,\n",
       "  22125,\n",
       "  292,\n",
       "  3815,\n",
       "  304,\n",
       "  12021,\n",
       "  278,\n",
       "  3186,\n",
       "  29915,\n",
       "  29879,\n",
       "  1880,\n",
       "  29899,\n",
       "  29567,\n",
       "  2472,\n",
       "  304,\n",
       "  278,\n",
       "  2305,\n",
       "  1058,\n",
       "  817,\n",
       "  372,\n",
       "  29889,\n",
       "  13,\n",
       "  1525,\n",
       "  5550,\n",
       "  1164,\n",
       "  5425,\n",
       "  29933,\n",
       "  6227,\n",
       "  1806,\n",
       "  29059,\n",
       "  13,\n",
       "  13,\n",
       "  2697,\n",
       "  16905,\n",
       "  322,\n",
       "  5925,\n",
       "  18112,\n",
       "  363,\n",
       "  319,\n",
       "  29902,\n",
       "  10606,\n",
       "  470,\n",
       "  322,\n",
       "  4891,\n",
       "  29899,\n",
       "  14318,\n",
       "  4933,\n",
       "  6509,\n",
       "  9279,\n",
       "  322,\n",
       "  2221,\n",
       "  304,\n",
       "  3275,\n",
       "  967,\n",
       "  5314,\n",
       "  13,\n",
       "  1576,\n",
       "  11509,\n",
       "  304,\n",
       "  4505,\n",
       "  4100,\n",
       "  5626,\n",
       "  310,\n",
       "  319,\n",
       "  29902,\n",
       "  10606,\n",
       "  3815,\n",
       "  515,\n",
       "  15281,\n",
       "  5687,\n",
       "  5849,\n",
       "  29892,\n",
       "  5314,\n",
       "  322,\n",
       "  13883,\n",
       "  304,\n",
       "  12021,\n",
       "  3234,\n",
       "  21556,\n",
       "  13,\n",
       "  29931,\n",
       "  1479,\n",
       "  4891,\n",
       "  29899,\n",
       "  14318,\n",
       "  9279,\n",
       "  304,\n",
       "  11157,\n",
       "  5680,\n",
       "  29914,\n",
       "  9794,\n",
       "  393,\n",
       "  14169,\n",
       "  5001,\n",
       "  9280,\n",
       "  29934,\n",
       "  13,\n",
       "  7648,\n",
       "  1598,\n",
       "  716,\n",
       "  18112,\n",
       "  363,\n",
       "  278,\n",
       "  3815,\n",
       "  304,\n",
       "  367,\n",
       "  13661,\n",
       "  8236,\n",
       "  297,\n",
       "  10161,\n",
       "  310,\n",
       "  7333,\n",
       "  1891,\n",
       "  20699,\n",
       "  322,\n",
       "  2793,\n",
       "  8004,\n",
       "  13,\n",
       "  797,\n",
       "  445,\n",
       "  2602,\n",
       "  29892,\n",
       "  366,\n",
       "  526,\n",
       "  3806,\n",
       "  304,\n",
       "  3667,\n",
       "  675,\n",
       "  596,\n",
       "  13661,\n",
       "  8236,\n",
       "  17924,\n",
       "  895,\n",
       "  373,\n",
       "  697,\n",
       "  470,\n",
       "  901,\n",
       "  310,\n",
       "  278,\n",
       "  1494,\n",
       "  390,\n",
       "  29987,\n",
       "  29928,\n",
       "  10161,\n",
       "  304,\n",
       "  3867,\n",
       "  28967,\n",
       "  7636,\n",
       "  6851,\n",
       "  470,\n",
       "  7136,\n",
       "  5722,\n",
       "  11763,\n",
       "  363,\n",
       "  4116,\n",
       "  442,\n",
       "  29328,\n",
       "  29303,\n",
       "  6757,\n",
       "  313,\n",
       "  3253,\n",
       "  29879,\n",
       "  29892,\n",
       "  10130,\n",
       "  29892,\n",
       "  2992,\n",
       "  467,\n",
       "  13,\n",
       "  13,\n",
       "  15263,\n",
       "  6189,\n",
       "  29257,\n",
       "  29892,\n",
       "  21784,\n",
       "  29257,\n",
       "  13,\n",
       "  29940,\n",
       "  18771,\n",
       "  17088,\n",
       "  10554,\n",
       "  292,\n",
       "  313,\n",
       "  10041,\n",
       "  19679,\n",
       "  29892,\n",
       "  11608,\n",
       "  2133,\n",
       "  29892,\n",
       "  1426,\n",
       "  23655,\n",
       "  29892,\n",
       "  2992,\n",
       "  29897,\n",
       "  13,\n",
       "  20606,\n",
       "  261,\n",
       "  478,\n",
       "  2459,\n",
       "  313,\n",
       "  3318,\n",
       "  19679,\n",
       "  29892,\n",
       "  1773,\n",
       "  326,\n",
       "  397,\n",
       "  284,\n",
       "  2793,\n",
       "  12965,\n",
       "  29892,\n",
       "  2992,\n",
       "  29897,\n",
       "  13,\n",
       "  29968,\n",
       "  3707,\n",
       "  5485,\n",
       "  12367,\n",
       "  13,\n",
       "  1123,\n",
       "  2055,\n",
       "  355,\n",
       "  362,\n",
       "  322,\n",
       "  24034,\n",
       "  14009,\n",
       "  13,\n",
       "  3629,\n",
       "  2107,\n",
       "  6042,\n",
       "  272,\n",
       "  304,\n",
       "  916,\n",
       "  4933,\n",
       "  6509,\n",
       "  9638,\n",
       "  2879,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  1123,\n",
       "  1548,\n",
       "  1860,\n",
       "  13,\n",
       "  13,\n",
       "  8140,\n",
       "  12539,\n",
       "  15146,\n",
       "  8232,\n",
       "  13,\n",
       "  13,\n",
       "  29906,\n",
       "  29974,\n",
       "  2440,\n",
       "  310,\n",
       "  7271,\n",
       "  297,\n",
       "  2874,\n",
       "  292,\n",
       "  322,\n",
       "  16049,\n",
       "  2106,\n",
       "  29899,\n",
       "  974,\n",
       "  29899,\n",
       "  1552,\n",
       "  29899,\n",
       "  442,\n",
       "  4933,\n",
       "  6509,\n",
       "  14009,\n",
       "  29892,\n",
       "  322,\n",
       "  15399,\n",
       "  963,\n",
       "  304,\n",
       "  1855,\n",
       "  3186,\n",
       "  4828,\n",
       "  13,\n",
       "  2568,\n",
       "  504,\n",
       "  719,\n",
       "  8236,\n",
       "  17924,\n",
       "  895,\n",
       "  297,\n",
       "  3058,\n",
       "  5354,\n",
       "  310,\n",
       "  4933,\n",
       "  6509,\n",
       "  13698,\n",
       "  29892,\n",
       "  7148,\n",
       "  297,\n",
       "  6483,\n",
       "  6509,\n",
       "  29892,\n",
       "  5613,\n",
       "  4086,\n",
       "  9068,\n",
       "  29892,\n",
       "  29303,\n",
       "  6757,\n",
       "  29892,\n",
       "  6601,\n",
       "  1998,\n",
       "  1080,\n",
       "  13,\n",
       "  17936,\n",
       "  2407,\n",
       "  310,\n",
       "  8472,\n",
       "  12021,\n",
       "  20414,\n",
       "  310,\n",
       "  5680,\n",
       "  29914,\n",
       "  9794,\n",
       "  304,\n",
       "  5802,\n",
       "  6757,\n",
       "  411,\n",
       "  1880,\n",
       "  10879,\n",
       "  491,\n",
       "  1985,\n",
       "  4822,\n",
       "  10907,\n",
       "  322,\n",
       "  25700,\n",
       "  13,\n",
       "  15403,\n",
       "  29884,\n",
       "  2556,\n",
       "  25964,\n",
       "  472,\n",
       "  2246,\n",
       "  13661,\n",
       "  378,\n",
       "  10662,\n",
       "  29914,\n",
       "  29926,\n",
       "  2905,\n",
       "  1338,\n",
       "  29892,\n",
       "  1532,\n",
       "  14831,\n",
       "  297,\n",
       "  278,\n",
       "  13661,\n",
       "  363,\n",
       "  278,\n",
       "  5354,\n",
       "  310,\n",
       "  17924,\n",
       "  895,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  6042,\n",
       "  943,\n",
       "  310,\n",
       "  16336,\n",
       "  4933,\n",
       "  6509,\n",
       "  9638,\n",
       "  2879,\n",
       "  322,\n",
       "  2221,\n",
       "  304,\n",
       "  6548,\n",
       "  963,\n",
       "  304,\n",
       "  278,\n",
       "  2446,\n",
       "  3233,\n",
       "  13,\n",
       "  18420,\n",
       "  3971,\n",
       "  322,\n",
       "  19182,\n",
       "  12084,\n",
       "  25078,\n",
       "  29892,\n",
       "  508,\n",
       "  664,\n",
       "  4822,\n",
       "  13303,\n",
       "  10907,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  14137,\n",
       "  25078,\n",
       "  297,\n",
       "  2999,\n",
       "  8720,\n",
       "  10276,\n",
       "  313,\n",
       "  29872,\n",
       "  29889,\n",
       "  29887,\n",
       "  29889,\n",
       "  3355,\n",
       "  29892,\n",
       "  315,\n",
       "  10024,\n",
       "  5132,\n",
       "  29892,\n",
       "  24419,\n",
       "  29897,\n",
       "  13,\n",
       "  13,\n",
       "  6572,\n",
       "  14373,\n",
       "  15146,\n",
       "  8232,\n",
       "  13,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  4066,\n",
       "  297,\n",
       "  9763,\n",
       "  5745,\n",
       "  322,\n",
       "  1749,\n",
       "  10655,\n",
       "  13,\n",
       "  5015,\n",
       "  549,\n",
       "  5354,\n",
       "  17924,\n",
       "  895,\n",
       "  297,\n",
       "  29303,\n",
       "  14009,\n",
       "  13,\n",
       "  29924,\n",
       "  29889,\n",
       "  29903,\n",
       "  470,\n",
       "  1963,\n",
       "  29889,\n",
       "  29928,\n",
       "  297,\n",
       "  6601,\n",
       "  10466,\n",
       "  29892,\n",
       "  23964,\n",
       "  29892,\n",
       "  17558,\n",
       "  470,\n",
       "  916,\n",
       "  4323,\n",
       "  23378,\n",
       "  4235,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  20841,\n",
       "  1389,\n",
       "  1169,\n",
       "  13,\n",
       "  13,\n",
       "  13,\n",
       "  6108,\n",
       "  537,\n",
       "  5134,\n",
       "  13,\n",
       "  29896,\n",
       "  29900,\n",
       "  29900,\n",
       "  29995,\n",
       "  16083,\n",
       "  29892,\n",
       "  12042,\n",
       "  284,\n",
       "  322,\n",
       "  18551,\n",
       "  1663,\n",
       "  18541,\n",
       "  23746,\n",
       "  313,\n",
       "  29896,\n",
       "  29900,\n",
       "  29900,\n",
       "  29995,\n",
       "  23746,\n",
       "  363,\n",
       "  8839,\n",
       "  1237,\n",
       "  29897,\n",
       "  13,\n",
       "  13953,\n",
       "  368,\n",
       "  27261,\n",
       "  29892,\n",
       "  844,\n",
       "  1082,\n",
       "  29892,\n",
       "  10426,\n",
       "  9008,\n",
       "  322,\n",
       "  330,\n",
       "  962,\n",
       "  2758,\n",
       "  2925,\n",
       "  13,\n",
       "  29946,\n",
       "  29900,\n",
       "  29896,\n",
       "  29895,\n",
       "  9686,\n",
       "  1824,\n",
       "  13,\n",
       "  20475,\n",
       "  301,\n",
       "  3322,\n",
       "  29892,\n",
       "  5807,\n",
       "  26514,\n",
       "  29892,\n",
       "  13748,\n",
       "  29879,\n",
       "  29892,\n",
       "  2992,\n",
       "  29889,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  4971,\n",
       "  6090,\n",
       "  29901,\n",
       "  8720,\n",
       "  29892,\n",
       "  302,\n",
       "  1896,\n",
       "  29895,\n",
       "  29892,\n",
       "  13023,\n",
       "  294,\n",
       "  29892,\n",
       "  26110,\n",
       "  29892,\n",
       "  26943,\n",
       "  326,\n",
       "  29892,\n",
       "  921,\n",
       "  29887,\n",
       "  17079,\n",
       "  29892,\n",
       "  16267,\n",
       "  29892,\n",
       "  544,\n",
       "  4778,\n",
       "  29892,\n",
       "  4802,\n",
       "  848,\n",
       "  29892,\n",
       "  9024,\n",
       "  262,\n",
       "  29892,\n",
       "  1426,\n",
       "  10054,\n",
       "  29892,\n",
       "  9570,\n",
       "  29892,\n",
       "  286,\n",
       "  29916,\n",
       "  1212,\n",
       "  29892,\n",
       "  2654,\n",
       "  29892,\n",
       "  805,\n",
       "  4135,\n",
       "  29892,\n",
       "  3578,\n",
       "  29887,\n",
       "  5838,\n",
       "  29892,\n",
       "  6483,\n",
       "  6509,\n",
       "  29892,\n",
       "  298,\n",
       "  3188,\n",
       "  29892,\n",
       "  750,\n",
       "  26793,\n",
       "  29892,\n",
       "  1591,\n",
       "  585,\n",
       "  29892,\n",
       "  330,\n",
       "  6814,\n",
       "  29892,\n",
       "  282,\n",
       "  3637,\n",
       "  25350,\n",
       "  29892,\n",
       "  286,\n",
       "  29880,\n",
       "  29892,\n",
       "  25879,\n",
       "  29892,\n",
       "  282,\n",
       "  335,\n",
       "  29892,\n",
       "  302,\n",
       "  22833,\n",
       "  29892,\n",
       "  4933,\n",
       "  6509,\n",
       "  29892,\n",
       "  10346,\n",
       "  29892,\n",
       "  3345,\n",
       "  362,\n",
       "  13,\n",
       "  2277,\n",
       "  29937,\n",
       "  2216,\n",
       "  4971,\n",
       "  6090,\n",
       "  29901,\n",
       "  12965,\n",
       "  29892,\n",
       "  12641,\n",
       "  29892,\n",
       "  12655,\n",
       "  29892,\n",
       "  4576,\n",
       "  29892,\n",
       "  16993,\n",
       "  3241,\n",
       "  29892,\n",
       "  1775,\n",
       "  8205,\n",
       "  29892,\n",
       "  848,\n",
       "  7604,\n",
       "  2133,\n",
       "  29892,\n",
       "  6601,\n",
       "  10466,\n",
       "  29892,\n",
       "  4560,\n",
       "  7354,\n",
       "  29899,\n",
       "  19668,\n",
       "  ...]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a distribution of our dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2217\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenize_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBk4Qp_vyRgh"
   },
   "source": [
    "From here, you can choose where you'd like to set the `max_length` to be. You can truncate and pad training examples to fit them to your chosen size. Be aware that choosing a larger `max_length` has its compute tradeoffs. \n",
    "\n",
    "I'm using my personal notes to train the model, and they vary greatly in length. I spent some time cleaning the dataset so the samples were about the same length, cutting up individual notes if needed, but being sure to not cut in the middle of a word or sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tokenize again with padding and truncation, and set up the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning is](https://neptune.ai/blog/self-supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 1024\n",
    "# def generate_and_tokenize_prompt(prompt):\n",
    "#     result = tokenizer(\n",
    "#         formatting_func(prompt),\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "# tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Check that `input_ids` is padded on the left with the `eos_token` (2) and there is an `eos_token` 2 added to the end, and the prompt starts with a `bos_token` (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '20315', 'skills': ['leadership', 'php', 'tensorflow', 'sql', 'c++', 'matlab', 'seaborn', 'event management', 'matplotlib', 'git', 'powerpoint', 'riak', 'c sharp', 'scikit-learn', 'excel', 'rstudio', 'jdbc', 'libsvm', 'writing', 'pytorch', 'javascript', 'python', 'latex', 'node.js', 'c#', 'jupyter notebook'], 'not_skills': ['keras', 'opencv', 'numpy', 'computer vision', 'anomaly detection', 'solidworks', 'cntk', 'adobe illustrator', 'pca', 'horovod', 'labview', 'maple', 'deep learning', 'selenium', 'svm', 'swift', 'cnn', 'adobe photoshop', 'igor pro', 'pandas', 'nlp', 'tensorfelow'], 'text': 'Located in the Bay Area, Geneus Tech Inc. is a subsidiary/branch research center of its parent company in China, which is dedicated to the development and commercial application of 4th generation (nanopore-based) gene sequencing solutions, including devices, IC-MEMS chips, reagents, algorithms and software systems. The core technology platform is based on the detection and processing of characteristic current signals generated by the interaction between nanopore biosensors and nucleic acid bases. Thus, high-throughput sequencing of nucleic acids can be achieved at the single-molecule level through a highly integrated chip system.\\nThis cutting-edge technology is highly expected to revolutionize the current sequencing filed by providing results with significantly higher data quality and lower cost as well as much higher convenience and timeliness. The parent company in China just hit a major milestone to launch the alpha version of our first generation product, making us highly encouraged and excited to continue our R&D, mature and upgrade the products, open up a blue ocean market and eventually reshape the current gene sequencing field to facilitate the new era of precision/personalized medicine. \\xa0 \\xa0\\nHere in the bay area research center, we are planning to attract talents interested in this frontier to take our R&D to a higher level. You will be provided with competitive salary, benefit and equity, as well as a bright career path and a warm, dynamic working environment.\\n\\xa0\\nResponsibilities:1. Develop, optimize and iterate algorithms to process raw current signals and convert them into nucleic acid sequences with high accuracy and efficiency.\\xa02. Develop, optimize and iterate algorithms for sequence alignment and obtain consensus sequence with high accuracy3. Advanced research of algorithm methodologies for single-molecule sequencing signals4. Work closely with the bioinformatics team and software team to integrate related', 'input_ids': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 17163, 12953, 29901, 5976, 630, 297, 278, 6211, 18320, 29892, 15350, 375, 1920, 305, 9266, 29889, 338, 263, 11684, 8819, 653, 29914, 17519, 5925, 4818, 310, 967, 3847, 5001, 297, 7551, 29892, 607, 338, 16955, 304, 278, 5849, 322, 12128, 2280, 310, 29871, 29946, 386, 12623, 313, 13707, 459, 487, 29899, 6707, 29897, 18530, 8617, 16750, 6851, 29892, 3704, 9224, 29892, 18340, 29899, 2303, 4345, 521, 4512, 29892, 337, 351, 1237, 29892, 14009, 322, 7047, 6757, 29889, 450, 7136, 15483, 7481, 338, 2729, 373, 278, 15326, 322, 9068, 310, 17443, 1857, 18470, 5759, 491, 278, 14881, 1546, 23432, 459, 487, 289, 2363, 575, 943, 322, 22699, 293, 22193, 22561, 29889, 6549, 29892, 1880, 29899, 20678, 649, 8617, 16750, 310, 22699, 293, 1274, 4841, 508, 367, 14363, 472, 278, 2323, 29899, 29885, 1772, 29883, 1297, 3233, 1549, 263, 10712, 23387, 29830, 1788, 29889, 13, 4013, 28967, 29899, 12864, 15483, 338, 10712, 3806, 304, 19479, 675, 278, 1857, 8617, 16750, 934, 29881, 491, 13138, 2582, 411, 16951, 6133, 848, 11029, 322, 5224, 3438, 408, 1532, 408, 1568, 6133, 29703, 322, 5335, 295, 3335, 29889, 450, 3847, 5001, 297, 7551, 925, 7124, 263, 4655, 2316, 27744, 304, 6826, 278, 15595, 1873, 310, 1749, 937, 12623, 3234, 29892, 3907, 502, 10712, 18443, 287, 322, 24173, 304, 6773, 1749, 390, 29987, 29928, 29892, 286, 1535, 322, 14955, 278, 9316, 29892, 1722, 701, 263, 7254, 23474, 9999, 322, 10201, 620, 14443, 278, 1857, 18530, 8617, 16750, 1746, 304, 16089, 10388, 278, 716, 3152, 310, 16716, 29914, 10532, 284, 1891, 26602, 29889, 20246, 20246, 13, 10605, 297, 278, 23041, 4038, 5925, 4818, 29892, 591, 526, 18987, 304, 13978, 5969, 1237, 8852, 297, 445, 4565, 631, 304, 2125, 1749, 390, 29987, 29928, 304, 263, 6133, 3233, 29889, 887, 674, 367, 4944, 411, 5100, 3321, 4497, 653, 29892, 14169, 322, 1592, 537, 29892, 408, 1532, 408, 263, 11785, 6413, 2224, 322, 263, 14294, 29892, 7343, 1985, 5177, 29889, 13, 30081, 13, 1666, 29886, 787, 747, 9770, 29901, 29896, 29889, 10682, 29892, 24656, 322, 13649, 14009, 304, 1889, 10650, 1857, 18470, 322, 3588, 963, 964, 22699, 293, 22193, 15602, 411, 1880, 13600, 322, 19201, 29889, 30081, 29906, 29889, 10682, 29892, 24656, 322, 13649, 14009, 363, 5665, 22239, 322, 4017, 1136, 8841, 5665, 411, 1880, 13600, 29941, 29889, 29287, 5925, 310, 5687, 1158, 11763, 363, 2323, 29899, 29885, 1772, 29883, 1297, 8617, 16750, 18470, 29946, 29889, 5244, 16467, 411, 278, 17799, 262, 4830, 1199, 3815, 322, 7047, 3815, 304, 22782, 4475, 13, 2277, 29937, 4971, 6090, 29901, 26001, 29892, 3989, 29892, 26110, 29892, 4576, 29892, 274, 10024, 1775, 8205, 29892, 409, 370, 1398, 29892, 1741, 10643, 29892, 22889, 29892, 6315, 29892, 3081, 3149, 29892, 364, 24061, 29892, 274, 15301, 29892, 4560, 7354, 29899, 19668, 29892, 10616, 29892, 364, 12073, 29892, 432, 11140, 29892, 4303, 4501, 29885, 29892, 5007, 29892, 282, 3637, 25350, 29892, 3513, 29892, 3017, 29892, 5683, 29916, 29892, 2943, 29889, 1315, 29892, 274, 6552, 432, 786, 25547, 451, 19273, 13, 2277, 29937, 2216, 4971, 6090, 29901, 13023, 294, 29892, 1722, 11023, 29892, 12655, 29892, 6601, 18551, 29892, 29342, 14997, 15326, 29892, 7773, 13129, 29892, 274, 593, 29895, 29892, 594, 16945, 8632, 1061, 29892, 282, 1113, 29892, 4029, 586, 397, 29892, 9775, 1493, 29892, 2910, 280, 29892, 6483, 6509, 29892, 18866, 29892, 3731, 29885, 29892, 12086, 29892, 274, 15755, 29892, 594, 16945, 6731, 10578, 459, 29892, 8919, 272, 410, 29892, 11701, 29892, 302, 22833, 29892, 25187, 4877, 295, 340, 2], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 835, 17163, 12953, 29901, 5976, 630, 297, 278, 6211, 18320, 29892, 15350, 375, 1920, 305, 9266, 29889, 338, 263, 11684, 8819, 653, 29914, 17519, 5925, 4818, 310, 967, 3847, 5001, 297, 7551, 29892, 607, 338, 16955, 304, 278, 5849, 322, 12128, 2280, 310, 29871, 29946, 386, 12623, 313, 13707, 459, 487, 29899, 6707, 29897, 18530, 8617, 16750, 6851, 29892, 3704, 9224, 29892, 18340, 29899, 2303, 4345, 521, 4512, 29892, 337, 351, 1237, 29892, 14009, 322, 7047, 6757, 29889, 450, 7136, 15483, 7481, 338, 2729, 373, 278, 15326, 322, 9068, 310, 17443, 1857, 18470, 5759, 491, 278, 14881, 1546, 23432, 459, 487, 289, 2363, 575, 943, 322, 22699, 293, 22193, 22561, 29889, 6549, 29892, 1880, 29899, 20678, 649, 8617, 16750, 310, 22699, 293, 1274, 4841, 508, 367, 14363, 472, 278, 2323, 29899, 29885, 1772, 29883, 1297, 3233, 1549, 263, 10712, 23387, 29830, 1788, 29889, 13, 4013, 28967, 29899, 12864, 15483, 338, 10712, 3806, 304, 19479, 675, 278, 1857, 8617, 16750, 934, 29881, 491, 13138, 2582, 411, 16951, 6133, 848, 11029, 322, 5224, 3438, 408, 1532, 408, 1568, 6133, 29703, 322, 5335, 295, 3335, 29889, 450, 3847, 5001, 297, 7551, 925, 7124, 263, 4655, 2316, 27744, 304, 6826, 278, 15595, 1873, 310, 1749, 937, 12623, 3234, 29892, 3907, 502, 10712, 18443, 287, 322, 24173, 304, 6773, 1749, 390, 29987, 29928, 29892, 286, 1535, 322, 14955, 278, 9316, 29892, 1722, 701, 263, 7254, 23474, 9999, 322, 10201, 620, 14443, 278, 1857, 18530, 8617, 16750, 1746, 304, 16089, 10388, 278, 716, 3152, 310, 16716, 29914, 10532, 284, 1891, 26602, 29889, 20246, 20246, 13, 10605, 297, 278, 23041, 4038, 5925, 4818, 29892, 591, 526, 18987, 304, 13978, 5969, 1237, 8852, 297, 445, 4565, 631, 304, 2125, 1749, 390, 29987, 29928, 304, 263, 6133, 3233, 29889, 887, 674, 367, 4944, 411, 5100, 3321, 4497, 653, 29892, 14169, 322, 1592, 537, 29892, 408, 1532, 408, 263, 11785, 6413, 2224, 322, 263, 14294, 29892, 7343, 1985, 5177, 29889, 13, 30081, 13, 1666, 29886, 787, 747, 9770, 29901, 29896, 29889, 10682, 29892, 24656, 322, 13649, 14009, 304, 1889, 10650, 1857, 18470, 322, 3588, 963, 964, 22699, 293, 22193, 15602, 411, 1880, 13600, 322, 19201, 29889, 30081, 29906, 29889, 10682, 29892, 24656, 322, 13649, 14009, 363, 5665, 22239, 322, 4017, 1136, 8841, 5665, 411, 1880, 13600, 29941, 29889, 29287, 5925, 310, 5687, 1158, 11763, 363, 2323, 29899, 29885, 1772, 29883, 1297, 8617, 16750, 18470, 29946, 29889, 5244, 16467, 411, 278, 17799, 262, 4830, 1199, 3815, 322, 7047, 3815, 304, 22782, 4475, 13, 2277, 29937, 4971, 6090, 29901, 26001, 29892, 3989, 29892, 26110, 29892, 4576, 29892, 274, 10024, 1775, 8205, 29892, 409, 370, 1398, 29892, 1741, 10643, 29892, 22889, 29892, 6315, 29892, 3081, 3149, 29892, 364, 24061, 29892, 274, 15301, 29892, 4560, 7354, 29899, 19668, 29892, 10616, 29892, 364, 12073, 29892, 432, 11140, 29892, 4303, 4501, 29885, 29892, 5007, 29892, 282, 3637, 25350, 29892, 3513, 29892, 3017, 29892, 5683, 29916, 29892, 2943, 29889, 1315, 29892, 274, 6552, 432, 786, 25547, 451, 19273, 13, 2277, 29937, 2216, 4971, 6090, 29901, 13023, 294, 29892, 1722, 11023, 29892, 12655, 29892, 6601, 18551, 29892, 29342, 14997, 15326, 29892, 7773, 13129, 29892, 274, 593, 29895, 29892, 594, 16945, 8632, 1061, 29892, 282, 1113, 29892, 4029, 586, 397, 29892, 9775, 1493, 29892, 2910, 280, 29892, 6483, 6509, 29892, 18866, 29892, 3731, 29885, 29892, 12086, 29892, 274, 15755, 29892, 594, 16945, 6731, 10578, 459, 29892, 8919, 272, 410, 29892, 11701, 29892, 302, 22833, 29892, 25187, 4877, 295, 340, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the samples should be the same length, `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "# eval_prompt = f\"\"\" ### The job description: {tokenized_train_dataset[1]['text']} \n",
    "\n",
    "# ### The skills:\n",
    "\n",
    "\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = f\"\"\" ### The job description: {tokenized_train_dataset[1]['text']} \n",
    "\n",
    "### The skills:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NidIuFXMyRgi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### The job description: Located in the Bay Area, Geneus Tech Inc. is a subsidiary/branch research center of its parent company in China, which is dedicated to the development and commercial application of 4th generation (nanopore-based) gene sequencing solutions, including devices, IC-MEMS chips, reagents, algorithms and software systems. The core technology platform is based on the detection and processing of characteristic current signals generated by the interaction between nanopore biosensors and nucleic acid bases. Thus, high-throughput sequencing of nucleic acids can be achieved at the single-molecule level through a highly integrated chip system.\n",
      "This cutting-edge technology is highly expected to revolutionize the current sequencing filed by providing results with significantly higher data quality and lower cost as well as much higher convenience and timeliness. The parent company in China just hit a major milestone to launch the alpha version of our first generation product, making us highly encouraged and excited to continue our R&D, mature and upgrade the products, open up a blue ocean market and eventually reshape the current gene sequencing field to facilitate the new era of precision/personalized medicine.    \n",
      "Here in the bay area research center, we are planning to attract talents interested in this frontier to take our R&D to a higher level. You will be provided with competitive salary, benefit and equity, as well as a bright career path and a warm, dynamic working environment.\n",
      " \n",
      "Responsibilities:1. Develop, optimize and iterate algorithms to process raw current signals and convert them into nucleic acid sequences with high accuracy and efficiency. 2. Develop, optimize and iterate algorithms for sequence alignment and obtain consensus sequence with high accuracy3. Advanced research of algorithm methodologies for single-molecule sequencing signals4. Work closely with the bioinformatics team and software team to integrate related \n",
      "\n",
      "### The skills:\n",
      "\n",
      "\n",
      " import { createAction } from 'redux-actions';\n",
      "import { combineReducers } from 'redux-reducers';\n",
      "import { combineEpics } from 'redux-epics';\n",
      "import { createStore } from 'redux-store';\n",
      "\n",
      "const ACTION_TYPE = 'ACTION_TYPE';\n",
      "\n",
      "const reducer = (state = {}, action) => {\n",
      "  switch (action.type) {\n",
      "    case ACTION_TYPE:\n",
      "      return action.payload;\n",
      "    default:\n",
      "      return state;\n",
      "  }\n",
      "};\n",
      "\n",
      "const epic = (action, store) => {\n",
      "  switch (action.type) {\n",
      "    case ACTION_TYPE:\n",
      "      return action.payload;\n",
      "    default:\n",
      "      return null;\n",
      "  }\n",
      "};\n",
      "\n",
      "const store = createStore(combineReducers({\n",
      "  reducer,\n",
      "  epic\n",
      "}));\n",
      "\n",
      "const action = createAction(ACTION_TYPE);\n",
      "\n",
      "store.dispatch(action({ payload: 'test' }));\n",
      "\n",
      "store.dispatch(action({ payload: 'test2' }));\n",
      "\n",
      "store.dispatch(action({ payload: 'test3\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 4. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9EUEDAl0ss3"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are `q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, and `lm_head`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XshGNsbxyRgj",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Here we define the LoRA config.\n",
    "\n",
    "`r` is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
    "\n",
    "`alpha` is the scaling factor for the learned weights. The weight matrix is scaled by `alpha/r`, and thus a higher value for `alpha` assigns more weight to the LoRA activations.\n",
    "\n",
    "The values used in the QLoRA paper were `r=64` and `lora_alpha=16`, and these are said to generalize well, but we will use `r=32` and `lora_alpha=64` so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ybeyl20n3dYH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26214400 || all params: 6698193920 || trainable%: 0.391365199531279\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        # \"v_proj\",\n",
    "        # \"o_proj\",\n",
    "        # \"gate_proj\",\n",
    "        # \"up_proj\",\n",
    "        # \"down_proj\",\n",
    "        # \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "See how the model looks different now, with the LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 5120)\n",
      "        (layers): ModuleList(\n",
      "          (0-39): 40 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (k_proj): Linear4bit(\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5120, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=5120, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              )\n",
      "              (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
      "              (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "\n",
    "Let's use Weights & Biases to track our training metrics. You'll need to apply an API key when prompted. Feel free to skip this if you'd like, and just comment out the `wandb` parameters in the `Trainer` definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliuchuning831\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"IPG-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 5. Run Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from transformers import Trainer\n",
    "\n",
    "\n",
    "# class CustomLoss(nn.Module):\n",
    "#     def __init__(self, weight_skills=1.5, weight_not_skills=0.5):\n",
    "#         super(CustomLoss, self).__init__()\n",
    "#         self.weight_skills = weight_skills\n",
    "#         self.weight_not_skills = weight_not_skills\n",
    "#         self.loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "#     def forward(self, logits, labels):\n",
    "#         # Flatten logits and labels for loss computation\n",
    "#         batch_size, sequence_length, num_labels = logits.shape\n",
    "#         logits_flat = logits.view(-1, num_labels)\n",
    "#         labels_flat = labels.view(-1)\n",
    "\n",
    "#         # Compute Cross-Entropy Loss without reduction\n",
    "#         loss = self.loss_fct(logits_flat, labels_flat)\n",
    "\n",
    "#         # Reshape loss back to (batch_size, sequence_length)\n",
    "#         loss = loss.view(batch_size, sequence_length)\n",
    "\n",
    "#         # Apply fixed weights to the loss\n",
    "#         skills_mask = labels == 1\n",
    "#         not_skills_mask = labels == 2\n",
    "#         loss[skills_mask] *= self.weight_skills\n",
    "#         loss[not_skills_mask] *= self.weight_not_skills\n",
    "\n",
    "#         # Average the loss\n",
    "#         return loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from transformers import Trainer\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get(\"logits\")\n",
    "\n",
    "#         # Cross-Entropy Loss\n",
    "#         loss_fct = nn.CrossEntropyLoss()\n",
    "#         batch_size, sequence_length, num_labels = logits.shape\n",
    "#         standard_loss = loss_fct(logits.view(batch_size * sequence_length, num_labels), labels.view(-1))\n",
    "\n",
    "#         # Custom logic for 'NotSkills'\n",
    "#         weight_not_skills = 2.0  # Consider adjusting based on class distribution\n",
    "#         not_skills_mask = labels == 0\n",
    "#         not_skills_loss = 1\n",
    "#         if not_skills_mask.any():\n",
    "#             not_skills_loss = loss_fct(logits[not_skills_mask], labels[not_skills_mask])\n",
    "\n",
    "#         # Combining losses\n",
    "#         total_loss = standard_loss + weight_not_skills * not_skills_loss\n",
    "\n",
    "#         return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import Trainer\n",
    "# import torch\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         # Initialize the custom loss function with desired weights\n",
    "#         self.custom_loss_fct = CustomLoss(weight_skills=2.0, weight_not_skills=0.0)\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         #print(inputs)\n",
    "#         #print(labels)\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # Call the custom loss function\n",
    "#         loss = self.custom_loss_fct(logits, labels)\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import Trainer\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         # Extract the outputs from the model\n",
    "#         outputs = model(**inputs)\n",
    "\n",
    "#         # The first element of model outputs is the logits\n",
    "#         logits = outputs.logits\n",
    "\n",
    "#         # Reshape labels to match the size of output\n",
    "#         labels = inputs[\"labels\"].view(-1)\n",
    "\n",
    "#         # Reshape logits to match the size of labels\n",
    "#         logits = logits.view(-1, self.model.config.num_labels)\n",
    "\n",
    "#         # Compute custom loss: Cross-Entropy Loss\n",
    "#         # Ignore the index of -100 in labels\n",
    "#         loss = F.cross_entropy(logits, labels, ignore_index=-100)\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class Trainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            # \"\"\"\n",
    "            # How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "            # Subclass and override for custom behavior.\n",
    "            # \"\"\"\n",
    "            # if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            #     labels = inputs.pop(\"labels\")\n",
    "            # else:\n",
    "            #     labels = None\n",
    "            # outputs = model(**inputs)\n",
    "            # print(inputs)\n",
    "            # # Save past state if it exists\n",
    "            # # TODO: this needs to be fixed and made cleaner later.\n",
    "            # if self.args.past_index >= 0:\n",
    "            #     self._past = outputs[self.args.past_index]\n",
    "\n",
    "            # if labels is not None:\n",
    "            #     unwrapped_model = unwrap_model(model)\n",
    "            #     if is_peft_available() and isinstance(unwrapped_model, PeftModel):\n",
    "            #         model_name = unwrapped_model.base_model.model._get_name()\n",
    "            #     else:\n",
    "            #         model_name = unwrapped_model._get_name()\n",
    "            #     if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "            #         loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            #     else:\n",
    "            #         loss = self.label_smoother(outputs, labels)\n",
    "            # else:\n",
    "            #     if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "            #         raise ValueError(\n",
    "            #             \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "            #             f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "            #         )\n",
    "            #     # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            #     loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "            # return (loss, outputs) if return_outputs else loss\n",
    "                \"\"\"\n",
    "            How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "            Subclass and override for custom behavior.\n",
    "            \"\"\"\n",
    "                outputs = model(**inputs)\n",
    "                # Save past state if it exists\n",
    "                # TODO: this needs to be fixed and made cleaner later.\n",
    "                if self.args.past_index >= 0:\n",
    "                    self._past = outputs[self.args.past_index]\n",
    "\n",
    "                if self.label_smoother is not None and \"labels\" in inputs:\n",
    "                    return self.label_smoother(outputs, inputs[\"labels\"])\n",
    "                else:\n",
    "                    # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "                    return outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "    # def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    #     # Extract the outputs from the model\n",
    "    #     outputs = model(**inputs)\n",
    "    #     logits = outputs.logits\n",
    "\n",
    "    #     # Debug: Print shapes before reshaping\n",
    "    #     print(\"Original Logits Shape:\", logits.shape)\n",
    "    #     print(\"Original Labels Shape:\", inputs[\"labels\"].shape)\n",
    "\n",
    "    #     # Flatten the logits and labels\n",
    "    #     logits = logits.view(-1, self.model.config.num_labels)  # Ensure this is set to 2 for binary classification\n",
    "    #     labels = inputs[\"labels\"].view(-1)\n",
    "\n",
    "    #     # Debug: Print shapes after reshaping\n",
    "    #     print(\"Reshaped Logits Shape:\", logits.shape)\n",
    "    #     print(\"Reshaped Labels Shape:\", labels.shape)\n",
    "\n",
    "    #     # Verify that the total number of elements matches\n",
    "    #     if logits.shape[0] != labels.shape[0]:\n",
    "    #         raise ValueError(f\"Mismatch in total number of elements: logits ({logits.shape[0]}) vs labels ({labels.shape[0]})\")\n",
    "\n",
    "    #     # Compute the loss\n",
    "    #     loss = F.cross_entropy(logits, labels, ignore_index=-100)\n",
    "    #     batch_size, sequence_length, num_labels = logits.shape\n",
    "\n",
    "    #     return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y110sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(transformers\u001b[39m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq0nX33BmfaC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/chuning/Downloads/llama/wandb/run-20231120_170325-gtq3ijlp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liuchuning831/IPG-finetune/runs/gtq3ijlp' target=\"_blank\">llama2-13b-IPG-finetune-2023-11-20-17-03</a></strong> to <a href='https://wandb.ai/liuchuning831/IPG-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liuchuning831/IPG-finetune' target=\"_blank\">https://wandb.ai/liuchuning831/IPG-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liuchuning831/IPG-finetune/runs/gtq3ijlp' target=\"_blank\">https://wandb.ai/liuchuning831/IPG-finetune/runs/gtq3ijlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chuning/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 23.66 GiB of which 448.19 MiB is free. Process 4455 has 287.47 MiB memory in use. Process 12793 has 9.51 GiB memory in use. Including non-PyTorch memory, this process has 12.12 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb Cell 56\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mtransformers\u001b[39m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1555\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1557\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1558\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1559\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1560\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py:1861\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1858\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1860\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1861\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1863\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1864\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1866\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1867\u001b[0m ):\n\u001b[1;32m   1868\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/trainer.py:2726\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2725\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2726\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2728\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2729\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "\u001b[1;32m/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chuning/Downloads/llama/llama2-IPG_finetune_mem.ipynb#Y106sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/utils/operations.py:662\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 662\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/utils/operations.py:650\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/peft/peft_model.py:1003\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    993\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    994\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    995\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   1001\u001b[0m         )\n\u001b[0;32m-> 1003\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m   1004\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1005\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1006\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1007\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1008\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1009\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1010\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1011\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m batch_size \u001b[39m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:106\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1035\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1036\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1037\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1038\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1039\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1040\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1041\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1042\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1043\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1044\u001b[0m )\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:912\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    909\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[idx] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 912\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[1;32m    915\u001b[0m         attention_mask,\n\u001b[1;32m    916\u001b[0m         position_ids,\n\u001b[1;32m    917\u001b[0m         past_key_value,\n\u001b[1;32m    918\u001b[0m         output_attentions,\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    923\u001b[0m         hidden_states,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    928\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    929\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_dynamo\u001b[39m.\u001b[39;49mdisable(fn, recursive)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:451\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m context_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m noop_context_fn \u001b[39mor\u001b[39;00m debug \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39muse_reentrant=False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     gen \u001b[39m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    454\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    455\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:230\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    227\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m    229\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 230\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    673\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    674\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    675\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    676\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    677\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    678\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    679\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:403\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[39mif\u001b[39;00m attention_mask\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39m1\u001b[39m, q_len, kv_seq_len):\n\u001b[1;32m    400\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention mask should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39m1\u001b[39m,\u001b[39m \u001b[39mq_len,\u001b[39m \u001b[39mkv_seq_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         )\n\u001b[0;32m--> 403\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39;49m attention_mask\n\u001b[1;32m    405\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[1;32m    406\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attn_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(query_states\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 23.66 GiB of which 448.19 MiB is free. Process 4455 has 287.47 MiB memory in use. Process 12793 has 9.51 GiB memory in use. Including non-PyTorch memory, this process has 12.12 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "\n",
    "def log_gpu_memory(log_file_path, interval=60):\n",
    "    gpu_memory_usage = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']).decode('utf-8')\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"GPU: {gpu_memory_usage}\\n\")\n",
    "\n",
    "    threading.Timer(interval, log_gpu_memory, args=[log_file_path, interval]).start()\n",
    "\n",
    "# 启动GPU内存监控\n",
    "log_file_path = 'gpu_memory_log.txt'\n",
    "log_gpu_memory(log_file_path)\n",
    "\n",
    "\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"IPG-finetune\"\n",
    "base_model_name = \"llama2-13b\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=4,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=3000,\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=100,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=100,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
